{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "983519bf-98a5-432b-b789-747e19a9639b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torchinfo import summary\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizerFast, DistilBertConfig, DistilBertModel\n",
        "\n",
        "# DistilBERT 모델용 tokenizer 로드 (pretrained)\n",
        "# 이 tokenizer는 문장을 토큰화해서 모델이 이해할 수 있는 input_ids로 변환해줌\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "7f7b74cf-c946-4da2-a199-f479caa59ab7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file path /Users/seungchanhong/.cache/kagglehub/datasets/debasisdotcom/name-entity-recognition-ner-dataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "# 데이터셋 다운로드\n",
        "import kagglehub\n",
        "\n",
        "file_path = \"debasisdotcom/name-entity-recognition-ner-dataset\"\n",
        "\n",
        "path = kagglehub.dataset_download(file_path)\n",
        "print(\"file path\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "id": "e8baa2be-63f9-4243-b4a3-83fff215d92c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "columns Index(['Sentence #', 'Word', 'POS', 'Tag'], dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td></td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1                          of   IN   O\n",
              "2               demonstrators  NNS   O\n",
              "3                        have  VBP   O\n",
              "4                     marched  VBN   O"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 데이터 확인하기\n",
        "\n",
        "# UnicodeDecodeError로 encoding 지정해줌 : ISO-8859-1, latin1 등\n",
        "df = pd.read_csv(path + '/NER dataset.csv', keep_default_na=False, encoding='ISO-8859-1')\n",
        "# head : 첫 5행 / columns : column 명 배열과 데이터 타입\n",
        "print(\"columns\", df.columns)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c25e999-20d3-4871-adac-73846a3b3128",
      "metadata": {},
      "source": [
        "### kaggle NER dataset 설명\n",
        "* Sentence : 주어진 문장\n",
        "* Word : 그 문장 속의 단어\n",
        "* POS : Part of Speech Tag - 각 단어의 품사 (명사/동사/형용사/부사 등)\n",
        "* Tag : Standard NER tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "fc88266e-6588-453c-a44d-0f73e34f8019",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prefix\n",
            "O    887908\n",
            "B    111891\n",
            "I     48776\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Tag에 대해 더 알아보기\n",
        "\n",
        "df['Prefix'] = df['Tag'].apply(lambda x: x.split('-')[0] if '-' in x else x)\n",
        "print(df['Prefix'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0be8ab-4776-4808-8e71-a13546afeed2",
      "metadata": {},
      "source": [
        "### BIO Tagging (Begin, Inside, Outside)\n",
        "* B-XXX : 문장 내에서 XXX type의 NER tag 중 가장 먼저 나온 것 \n",
        "* I-XXX : 문장 내에서 XXX type의 NER tag 중 두 번째부터 ~ 나머지\n",
        "* O : NER tag에 속하지 않는 모든 단어  \n",
        "dataset을 보면 O가 가장 많음  \n",
        "그 다음으로는 특정 type에서 한 번만 나와도 B로 tagginge될 것이기 때문에 B가 많고 I 순\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "id": "6bd53fd6-0a15-485e-ac92-fed142303282",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prefix\n",
            "NN      145807\n",
            "NNP     131426\n",
            "IN      120996\n",
            "DT       98454\n",
            "JJ       78412\n",
            "NNS      75840\n",
            ".        47831\n",
            "VBD      39379\n",
            ",        32757\n",
            "VBN      32328\n",
            "VBZ      24960\n",
            "CD       24695\n",
            "VB       24211\n",
            "CC       23716\n",
            "TO       23061\n",
            "RB       20252\n",
            "VBG      19125\n",
            "VBP      16158\n",
            "PRP      13318\n",
            "POS      11257\n",
            "PRP$      8655\n",
            "MD        6973\n",
            "``        3728\n",
            "WDT       3698\n",
            "JJS       3034\n",
            "JJR       2967\n",
            "WP        2542\n",
            "NNPS      2521\n",
            "RP        2490\n",
            "WRB       2184\n",
            "$         1149\n",
            "RBR       1055\n",
            ":          795\n",
            "RRB        679\n",
            "LRB        678\n",
            "EX         663\n",
            "RBS        296\n",
            ";          214\n",
            "PDT        147\n",
            "WP$         99\n",
            "UH          24\n",
            "FW           1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df['Prefix'] = df['POS'].apply(lambda x: x.splt('-')[0] if '-' in x else x)\n",
        "print(df['Prefix'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bde5fe1-283a-4ad3-a163-fd599fd8067a",
      "metadata": {},
      "source": [
        "## POS tag : 단어의 품사\n",
        "* NN (Noun) : 명사  /  VB (Verb) : 동사  /  JJ (Adjective) : 형용사  /  RB (Adverb) : 부사\n",
        "* PRP (Pronoun) : 대명사  /  CC (Conjunction) : 접속사  /  IN (Preposition) : 전치사  /  UH (Interjection) : 감탄사\n",
        "* 아무래도 명사 NN, 대명사 PRP, 전치사 IN이 주를 이룸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "id": "caad3518-e899-4619-9508-f99834c252ff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique_tags ['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim', 'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve', 'I-eve', 'I-nat']\n",
            "\n",
            "tag2id {'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n",
            "\n",
            "id2tag {0: 'O', 1: 'B-geo', 2: 'B-gpe', 3: 'B-per', 4: 'I-geo', 5: 'B-org', 6: 'I-org', 7: 'B-tim', 8: 'B-art', 9: 'I-art', 10: 'I-per', 11: 'I-gpe', 12: 'I-tim', 13: 'B-nat', 14: 'B-eve', 15: 'I-eve', 16: 'I-nat'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# data labeling 하기\n",
        "# NER의 학습 목표가 어떤 entity인지 예측하는 것이기 때문에 Tag를 label로 삼기\n",
        "\n",
        "# unique한 label만 선별\n",
        "unique_tags = df['Tag'].unique().tolist()\n",
        "print(\"unique_tags\", unique_tags)\n",
        "print()\n",
        "\n",
        "# tag를 숫자로 mapping\n",
        "tag2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "print(\"tag2id\", tag2id)\n",
        "print()\n",
        "\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}\n",
        "print(\"id2tag\", id2tag)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "9266f97d-c79f-43be-9d6d-c632b6a09e2d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장 수 47960\n",
            "첫 문장 {'tokens': ['Thousands'], 'labels': [0]}\n"
          ]
        }
      ],
      "source": [
        "# 문장 단위로 group 만들기\n",
        "from collections import defaultdict\n",
        "\n",
        "def group_by_sentence(df, tag2id):\n",
        "    grouped = defaultdict(lambda: {\"tokens\": [], \"labels\": []})\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        sentence_id = row['Sentence #']\n",
        "        word = row['Word']\n",
        "        tag = row['Tag']\n",
        "\n",
        "        grouped[sentence_id]['tokens'].append(word)\n",
        "        grouped[sentence_id]['labels'].append(tag2id[tag])\n",
        "\n",
        "    return list(grouped.values())\n",
        "\n",
        "ner_data = group_by_sentence(df, tag2id)\n",
        "\n",
        "# 예시\n",
        "print(\"문장 수\", len(ner_data))\n",
        "print(\"첫 문장\", ner_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "id": "b98d4d14-b095-47d7-9f01-edcbfa724666",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128])\n",
            "tensor([-100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100, -100, -100, -100, -100])\n"
          ]
        }
      ],
      "source": [
        "def tokenize_and_align_labels(tokenizer, tokens, labels, max_len=128):\n",
        "    # tokenizer는 is_split_into_words=True 옵션 필요\n",
        "    encoding = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # 토큰마다 어떤 단어에서 온건지\n",
        "    word_ids = encoding.word_ids(batch_index=0)\n",
        "\n",
        "    aligned_labels = []\n",
        "    previous_word_idx = None\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            aligned_labels.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            aligned_labels.append(labels[word_idx])\n",
        "        else:\n",
        "            aligned_labels.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    encoding['labels'] = torch.tensor(aligned_labels)\n",
        "    return encoding\n",
        "\n",
        "# 예시\n",
        "example = samples[0]\n",
        "encoded = tokenize_and_align_labels(tokenizer, example['tokens'], example['labels'])\n",
        "\n",
        "print(encoded['input_ids'].shape)\n",
        "print(encoded['labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd178d3-a81b-4b41-a233-7b6e6f2a0312",
      "metadata": {},
      "source": [
        "* -100을 넣는 이유\n",
        "  label은 단어 단위로 있는데, 단어를 잘게 나누면 한 개의 단어가 2개의 토큰으로 보일 수 있음\n",
        "  이때, 첫 번째 토큰을 제외한 나머지 토큰에 -100을 부여"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "id": "9dc99d69-9b0e-429e-86a6-6789e2591059",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# NER dataset 정의\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, tag2id, max_len=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tag2id = tag2id\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        tokens = item['tokens']\n",
        "        labels = item['labels']\n",
        "        \n",
        "        encoded = tokenize_and_align_labels(\n",
        "            self.tokenizer, tokens, labels, max_len=self.max_len\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoded['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(0),\n",
        "            'labels': encoded['labels']\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "67dafea2-e31c-43a2-b293-5cbe9384b1b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train, test split\n",
        "train_data, test_data = train_test_split(ner_data, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataset = NERDataset(train_data, tokenizer, tag2id)\n",
        "test_dataset = NERDataset(test_data, tokenizer, tag2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "503edfbe-7427-4fb2-b14e-5b0791fd3261",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# data loader 구성\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "88a98a29-2037-4b01-af23-6ef216c0fb37",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DistilBertConfig, DistilBertForTokenClassification\n",
        "\n",
        "# 모델 생성 함수\n",
        "def get_ner_model(pretrained, num_labels):\n",
        "    if pretrained:\n",
        "        return DistilBertForTokenClassification.from_pretrained(\n",
        "            'distilbert-base-uncased',\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "    else:\n",
        "        config = DistilBertConfig(num_labels=num_labels)\n",
        "        return DistilBertForTokenClassification(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "id": "22530c05-8727-4af2-95bd-6ad846c782f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "def train_model(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "cf641d0f-06c0-4d22-96ea-5ac4911c6a78",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 정확도 측정 함수\n",
        "def accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_tokens, correct_tokens = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "            # loss 계산에서 제외한 토큰(-100)은 평가에서도 제외\n",
        "            mask = labels != -100\n",
        "            total_tokens += mask.sum().item()\n",
        "            correct_tokens += ((predictions == labels) & mask).sum().item()\n",
        "\n",
        "    return correct_tokens / total_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "id": "e01279d9-3acc-424b-b7f2-d82b23bf1d0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_comparison(result1, result2, label1='Pretrained', label2='Untrained'):\n",
        "    epochs = list(range(1, len(result1['train_losses']) + 1))\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Loss 비교\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, result1['train_losses'], marker='o', label=f'{label1} Loss')\n",
        "    plt.plot(epochs, result2['train_losses'], marker='x', label=f'{label2} Loss')\n",
        "    plt.title('Train Loss Comparison')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy 비교\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, result1['test_accs'], marker='o', label=f'{label1} Test Acc')\n",
        "    plt.plot(epochs, result2['test_accs'], marker='x', label=f'{label2} Test Acc')\n",
        "    plt.title('Test Accuracy Comparison')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.suptitle('Pretrained vs Untrained Comparison', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "id": "093f20d7-f189-412d-b5a2-ee2c8b4d1585",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "id": "018aa32b-e6d2-47f6-940b-1e0f2779d5e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_experiment(pretrained=True, lr=5e-5, epochs=5, weight_decay=0.01, device=device):\n",
        "    print(f\"\\n▶ Running {'[Pretrained]' if pretrained else '[Untrained]'} model \"\n",
        "          f\"for {epochs} epochs | lr={lr}, weight_decay={weight_decay}\")\n",
        "\n",
        "    # 모델 구성\n",
        "    model = get_ner_model(pretrained, num_labels=len(tag2id)).to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train_model(model, train_loader, optimizer, device)\n",
        "        train_acc = accuracy(model, train_loader, device)\n",
        "        test_acc = accuracy(model, test_loader, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch:2d} | Loss: {train_loss:.4f} | \"\n",
        "              f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    plot_comparison(train_losses, train_accs, test_accs,\n",
        "                 title=f\"{'Pretrained' if pretrained else 'Untrained'} | lr={lr} | Epochs={epochs}\")\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'test_accs': test_accs\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "id": "ded989d5-4f09-456f-b0ee-d07b4e47ccef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "▶ Running [Pretrained] model for 5 epochs | lr=5e-05, weight_decay=0.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[230], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_pretrained \u001b[38;5;241m=\u001b[39m run_experiment(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m result_untrained \u001b[38;5;241m=\u001b[39m run_experiment(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "Cell \u001b[0;32mIn[228], line 14\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(pretrained, lr, epochs, weight_decay, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m test_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, optimizer, device)\n\u001b[1;32m     15\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m accuracy(model, train_loader, device)\n\u001b[1;32m     16\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m accuracy(model, test_loader, device)\n",
            "Cell \u001b[0;32mIn[214], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week3/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week3/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week3/lib/python3.13/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     adam(\n\u001b[1;32m    245\u001b[0m         params_with_grad,\n\u001b[1;32m    246\u001b[0m         grads,\n\u001b[1;32m    247\u001b[0m         exp_avgs,\n\u001b[1;32m    248\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    249\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    250\u001b[0m         state_steps,\n\u001b[1;32m    251\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    252\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    253\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    254\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    255\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    256\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    257\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    258\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    259\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    260\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    261\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    262\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    263\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    264\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week3/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week3/lib/python3.13/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m func(\n\u001b[1;32m    877\u001b[0m     params,\n\u001b[1;32m    878\u001b[0m     grads,\n\u001b[1;32m    879\u001b[0m     exp_avgs,\n\u001b[1;32m    880\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    881\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    882\u001b[0m     state_steps,\n\u001b[1;32m    883\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    884\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    885\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    886\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    887\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    888\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    889\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    890\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    891\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    892\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    893\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    894\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    895\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week3/lib/python3.13/site-packages/torch/optim/adam.py:398\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    395\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[1;32m    401\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "result_pretrained = run_experiment(pretrained=True, lr=5e-5, epochs=5, device=device)\n",
        "result_untrained = run_experiment(pretrained=False, lr=5e-5, epochs=5, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ee1534-e2fb-4ff8-b4f5-b579d1661343",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (hhplus_week3)",
      "language": "python",
      "name": "hhplus_week3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adedbeb2-485c-477d-9196-92a85fc63727",
   "metadata": {},
   "source": [
    "## 과제 요약\n",
    "### [1] PDF를 load하는 라이브러리를 사용\n",
    "\n",
    "### 2. prompt를 custom하여 결과 도출\n",
    "#### i) rlm/rag-prompt는 최대 3문장이 기본 설정값\n",
    "#### ii) custom prompt\n",
    "##### - 전체 수상작을 포함시키도록 수정\n",
    "##### - 각 수상작에 대해 1문장으로 요약하도록 설정\n",
    "##### - 최대 8문장으로 상향 조정\n",
    "##### - 이어지는 문단으로 기술할 것\n",
    "##### - 한글로 요약하되 프로젝트명이 영어이면 영어로 표기\n",
    "\n",
    "### [2] custom_prompt 설정\n",
    "#### 1. 논문을 요약 전문가로 설정\n",
    "#### 2. 요약에 포함할 내용\n",
    "##### 1) 논문의 의미\n",
    "##### 2) 논문의 method나 접근 방법\n",
    "##### 3) 주요 발견 및 결과\n",
    "##### 4) 논문의 인사이트\n",
    "#### 3. 문맥에 없는 사실을 만들어내지 말 것\n",
    "#### 4. reference나 인용은 포함시키지 말 것\n",
    "#### 5. 명확하고 학술적인 언어로 작성할 것\n",
    "\n",
    "### [3] 요약 결과\n",
    "#### 1) RAG를 도입한 모델 설명\n",
    "#### 2) method : retrieval mechanism을 이용한 답변 생성\n",
    "#### 3) 결과 : RAG를 이용하여 독해 benchmark에서 뛰어난 성능을 보임\n",
    "#### 4) insight : 복잡도를 크게 증가시키지 않고도 자연어 처리를 잘 할 수 있음을 시사\n",
    "\n",
    "The paper presents a novel approach to multi-paragraph reading comprehension by introducing the RAG (Retrieval-Augmented Generation) model, which effectively combines parametric and non-parametric memory components to improve performance in tasks requiring contextual understanding. The proposed methodology utilizes retrieval mechanisms to enhance the generation of responses based on external knowledge sources, allowing for better contextual relevance and accuracy. The key findings demonstrate that RAG achieves competitive results in reading comprehension benchmarks, outperforming traditional models without needing complex pipeline architectures or extensive retrieval supervision. Notably, the work highlights the potential of integrating retrieval methods into generative models, which can lead to advances in various natural language processing applications by leveraging external knowledge efficiently without significant increases in complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfbc94-eb17-4b9f-a821-4e37e6d9d76b",
   "metadata": {},
   "source": [
    "## 필요한 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6fffc36-001a-4eec-81c3-6278d36d39a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d620e76b-5d78-408f-ad4e-87841d344b3e",
   "metadata": {},
   "source": [
    "## 환경 변수 가져와서 API key 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18e5160-b859-4e24-a3c1-9dede23faea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "langsmith_api_key = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "langsmith_endpoint = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = langsmith_endpoint\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808438fb-16fe-4c6b-bf0f-caea832dc274",
   "metadata": {},
   "source": [
    "## 모델 설정 : gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0cbb30-77f6-476f-a9c0-12379f5a6c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
    "client = Client(api_key=langsmith_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc443c96-3ae5-4945-84c0-6a4f14cab5bf",
   "metadata": {},
   "source": [
    "## 논문 파일 가져와서 로드하기\n",
    "\n",
    "### PDFPlumberLoader\n",
    " #### file_path\n",
    "  - load할 file 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f4b724d-34e9-4f5a-a87b-d62e2932e92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "loader = PDFPlumberLoader(\n",
    "    file_path=\"./2005.11401.pdf\",\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fafda939-1c12-4f4a-b00c-5744cfd2df96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 0, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrickLewis†‡,EthanPerez(cid:63),\\nAleksandraPiktus†,FabioPetroni†,VladimirKarpukhin†,NamanGoyal†,HeinrichKüttler†,\\nMikeLewis†,Wen-tauYih†,TimRocktäschel†‡,SebastianRiedel†‡,DouweKiela†\\n†FacebookAIResearch;‡UniversityCollegeLondon;(cid:63)NewYorkUniversity;\\nplewis@fb.com\\nAbstract\\nLargepre-trainedlanguagemodelshavebeenshowntostorefactualknowledge\\nintheirparameters,andachievestate-of-the-artresultswhenfine-tunedondown-\\nstreamNLPtasks. However,theirabilitytoaccessandpreciselymanipulateknowl-\\nedgeisstilllimited,andhenceonknowledge-intensivetasks,theirperformance\\nlagsbehindtask-specificarchitectures.Additionally,providingprovenancefortheir\\ndecisionsandupdatingtheirworldknowledgeremainopenresearchproblems. Pre-\\ntrainedmodelswithadifferentiableaccessmechanismtoexplicitnon-parametric\\nmemoryhavesofarbeenonlyinvestigatedforextractivedownstreamtasks. We\\nexploreageneral-purposefine-tuningrecipeforretrieval-augmentedgeneration\\n(RAG)—modelswhichcombinepre-trainedparametricandnon-parametricmem-\\nory for language generation. We introduce RAG models where the parametric\\nmemoryisapre-trainedseq2seqmodelandthenon-parametricmemoryisadense\\nvectorindexofWikipedia,accessedwithapre-trainedneuralretriever. Wecom-\\nparetwoRAGformulations,onewhichconditionsonthesameretrievedpassages\\nacrossthewholegeneratedsequence,andanotherwhichcanusedifferentpassages\\npertoken. Wefine-tuneandevaluateourmodelsonawiderangeofknowledge-\\nintensiveNLPtasksandsetthestateoftheartonthreeopendomainQAtasks,\\noutperformingparametricseq2seqmodelsandtask-specificretrieve-and-extract\\narchitectures. Forlanguagegenerationtasks,wefindthatRAGmodelsgenerate\\nmorespecific,diverseandfactuallanguagethanastate-of-the-artparametric-only\\nseq2seqbaseline.\\n1 Introduction\\nPre-trainedneurallanguagemodelshavebeenshowntolearnasubstantialamountofin-depthknowl-\\nedgefromdata[47]. Theycandosowithoutanyaccesstoanexternalmemory,asaparameterized\\nimplicitknowledgebase[51,52]. Whilethisdevelopmentisexciting,suchmodelsdohavedown-\\nsides: Theycannoteasilyexpandorrevisetheirmemory,can’tstraightforwardlyprovideinsightinto\\ntheirpredictions,andmayproduce“hallucinations”[38]. Hybridmodelsthatcombineparametric\\nmemorywithnon-parametric(i.e.,retrieval-based)memories[20,26,48]canaddresssomeofthese\\nissuesbecauseknowledgecanbedirectlyrevisedandexpanded,andaccessedknowledgecanbe\\ninspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that\\ncombinemaskedlanguagemodels[8]withadifferentiableretriever,haveshownpromisingresults,\\n1202\\nrpA\\n21\\n]LC.sc[\\n4v10411.5002:viXra\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 1, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Define \"middle ear\"(x) The middle ear includes\\nEnd-to-End Backprop through q and p θ the tympanic cavity and\\nQuestion Answering: the three ossicles. (y)\\nQuestion Query\\nE\\nQ\\nnc\\nu\\no\\ne\\nd\\nry\\ner\\nRetriever p\\nη\\nDo\\nI\\nc\\nn\\nu\\nd\\nm\\nex\\nent Generator pθ Q\\nA\\nu\\nn\\ne\\ns\\ns\\nw\\nti\\ne\\no\\nr\\nn\\nG\\nA\\ne\\nn\\nn\\ns\\ne\\nw\\nra\\ne\\nt\\nr\\ni\\ni\\no\\nn\\nn\\ng:\\n(Non-Parametric) (Parametric)\\nB b a o r r a n c k i n O b H a a m w a a i w i a .( s x) q(x) d(z) z4 supports (y)\\nFact Verification: Fact Query z3 Margin- Fact Verification:\\nz2 alize Label Generation\\nT C h o e m e D d i y v ( i x n ) e q MIPS z1 p θ T i h s i s d i 1 v 4 i t d h e d c e i n n t t u o r y 3 work\\nJeopardy Question sections: \"Inferno\",\\nGeneration: \"Purgatorio\" &\\nAnswer Query \"Paradiso\" (y)\\nQuestion Generation\\nFigure1:Overviewofourapproach.Wecombineapre-trainedretriever(QueryEncoder+Document\\nIndex)withapre-trainedseq2seqmodel(Generator)andfine-tuneend-to-end. Forqueryx,weuse\\nMaximumInnerProductSearch(MIPS)tofindthetop-Kdocumentsz . Forfinalpredictiony,we\\ni\\ntreatzasalatentvariableandmarginalizeoverseq2seqpredictionsgivendifferentdocuments.\\nbuthaveonlyexploredopen-domainextractivequestionanswering. Here,webringhybridparametric\\nandnon-parametricmemorytothe“workhorseofNLP,”i.e. sequence-to-sequence(seq2seq)models.\\nWeendowpre-trained,parametric-memorygenerationmodelswithanon-parametricmemorythrough\\nageneral-purposefine-tuningapproachwhichwerefertoasretrieval-augmentedgeneration(RAG).\\nWebuildRAGmodelswheretheparametricmemoryisapre-trainedseq2seqtransformer,andthe\\nnon-parametricmemoryisadensevectorindexofWikipedia,accessedwithapre-trainedneural\\nretriever. Wecombinethesecomponentsinaprobabilisticmodeltrainedend-to-end(Fig. 1). The\\nretriever(DensePassageRetriever[26],henceforthDPR)provideslatentdocumentsconditionedon\\ntheinput,andtheseq2seqmodel(BART[32])thenconditionsontheselatentdocumentstogetherwith\\ntheinputtogeneratetheoutput. Wemarginalizethelatentdocumentswithatop-Kapproximation,\\neitheronaper-outputbasis(assumingthesamedocumentisresponsibleforalltokens)oraper-token\\nbasis(wheredifferentdocumentsareresponsiblefordifferenttokens). LikeT5[51]orBART,RAG\\ncanbefine-tunedonanyseq2seqtask,wherebyboththegeneratorandretrieverarejointlylearned.\\nTherehasbeenextensivepreviousworkproposingarchitecturestoenrichsystemswithnon-parametric\\nmemorywhicharetrainedfromscratchforspecifictasks, e.g. memorynetworks[64,55], stack-\\naugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both\\nparametricandnon-parametricmemorycomponentsarepre-trainedandpre-loadedwithextensive\\nknowledge. Crucially,byusingpre-trainedaccessmechanisms,theabilitytoaccessknowledgeis\\npresentwithoutadditionaltraining.\\nOurresultshighlightthebenefitsofcombiningparametricandnon-parametricmemorywithgenera-\\ntionforknowledge-intensivetasks—tasksthathumanscouldnotreasonablybeexpectedtoperform\\nwithoutaccesstoanexternalknowledgesource. OurRAGmodelsachievestate-of-the-artresults\\nonopenNaturalQuestions[29],WebQuestions[3]andCuratedTrec[2]andstronglyoutperform\\nrecentapproachesthatusespecialisedpre-trainingobjectivesonTriviaQA[24]. Despitethesebeing\\nextractivetasks,wefindthatunconstrainedgenerationoutperformspreviousextractiveapproaches.\\nForknowledge-intensivegeneration,weexperimentwithMS-MARCO[1]andJeopardyquestion\\ngeneration, and we find that our models generate responses that are more factual, specific, and\\ndiversethanaBARTbaseline. ForFEVER[56]factverification,weachieveresultswithin4.3%of\\nstate-of-the-artpipelinemodelswhichusestrongretrievalsupervision. Finally,wedemonstratethat\\nthenon-parametricmemorycanbereplacedtoupdatethemodels’knowledgeastheworldchanges.1\\n2 Methods\\nWeexploreRAGmodels,whichusetheinputsequencextoretrievetextdocumentszandusethem\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleveragetwocomponents: (i)aretrieverp (z|x)withparametersη thatreturns(top-Ktruncated)\\nη\\ndistributionsovertextpassagesgivenaqueryxand(ii)ageneratorp (y |x,z,y )parametrized\\nθ i 1:i−1\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\nersLibrary[66]andcanbefoundathttps://github.com/huggingface/transformers/blob/master/\\nexamples/rag/.AninteractivedemoofRAGmodelscanbefoundathttps://huggingface.co/rag/\\n2\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 2, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='byθthatgeneratesacurrenttokenbasedonacontextofthepreviousi−1tokensy ,theoriginal\\n1:i−1\\ninputxandaretrievedpassagez.\\nTotraintheretrieverandgeneratorend-to-end,wetreattheretrieveddocumentasalatentvariable.\\nWeproposetwomodelsthatmarginalizeoverthelatentdocumentsindifferentwaystoproducea\\ndistributionovergeneratedtext. Inoneapproach,RAG-Sequence,themodelusesthesamedocument\\ntopredicteachtargettoken. Thesecondapproach,RAG-Token,canpredicteachtargettokenbased\\nonadifferentdocument. Inthefollowing,weformallyintroducebothmodelsandthendescribethe\\np andp components,aswellasthetraininganddecodingprocedure.\\nη θ\\n2.1 Models\\nRAG-SequenceModel TheRAG-Sequencemodelusesthesameretrieveddocumenttogenerate\\nthecompletesequence. Technically,ittreatstheretrieveddocumentasasinglelatentvariablethat\\nismarginalizedtogettheseq2seqprobabilityp(y|x)viaatop-Kapproximation. Concretely,the\\ntopKdocumentsareretrievedusingtheretriever,andthegeneratorproducestheoutputsequence\\nprobabilityforeachdocument,whicharethenmarginalized,\\nN\\n(cid:88) (cid:88) (cid:89)\\np (y|x) ≈ p (z|x)p (y|x,z) = p (z|x) p (y |x,z,y )\\nRAG-Sequence η θ η θ i 1:i−1\\nz∈top-k(p(·|x)) z∈top-k(p(·|x)) i\\nRAG-TokenModel IntheRAG-Tokenmodelwecandrawadifferentlatentdocumentforeach\\ntargettokenandmarginalizeaccordingly. Thisallowsthegeneratortochoosecontentfromseveral\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever,andthenthegeneratorproducesadistributionforthenextoutputtokenforeachdocument,\\nbeforemarginalizing,andrepeatingtheprocesswiththefollowingoutputtoken,Formally,wedefine:\\nN\\n(cid:89) (cid:88)\\np (y|x) ≈ p (z|x)p (y |x,z,y )\\nRAG-Token η θ i 1:i−1\\ni z∈top-k(p(·|x))\\nFinally,wenotethatRAGcanbeusedforsequenceclassificationtasksbyconsideringthetargetclass\\nasatargetsequenceoflengthone,inwhichcaseRAG-SequenceandRAG-Tokenareequivalent.\\n2.2 Retriever: DPR\\nTheretrievalcomponentp (z|x)isbasedonDPR[26]. DPRfollowsabi-encoderarchitecture:\\nη\\np (z|x)∝exp (cid:0) d(z)(cid:62)q(x) (cid:1) d(z)=BERT (z), q(x)=BERT (x)\\nη d q\\nwhered(z)isadenserepresentationofadocumentproducedbyaBERT documentencoder[8],\\nBASE\\nandq(x)aqueryrepresentationproducedbyaqueryencoder,alsobasedonBERT . Calculating\\nBASE\\ntop-k(p (·|x)),thelistofkdocumentszwithhighestpriorprobabilityp (z|x),isaMaximumInner\\nη η\\nProductSearch(MIPS)problem,whichcanbeapproximatelysolvedinsub-lineartime[23]. Weuse\\napre-trainedbi-encoderfromDPRtoinitializeourretrieverandtobuildthedocumentindex. This\\nretrieverwastrainedtoretrievedocumentswhichcontainanswerstoTriviaQA[24]questionsand\\nNaturalQuestions[29]. Werefertothedocumentindexasthenon-parametricmemory.\\n2.3 Generator: BART\\nThegeneratorcomponentp (y |x,z,y )couldbemodelledusinganyencoder-decoder. Weuse\\nθ i 1:i−1\\nBART-large[32],apre-trainedseq2seqtransformer[58]with400Mparameters.Tocombinetheinput\\nxwiththeretrievedcontentzwhengeneratingfromBART,wesimplyconcatenatethem. BARTwas\\npre-trainedusingadenoisingobjectiveandavarietyofdifferentnoisingfunctions. Ithasobtained\\nstate-of-the-artresultsonadiversesetofgenerationtasksandoutperformscomparably-sizedT5\\nmodels[32]. WerefertotheBARTgeneratorparametersθastheparametricmemoryhenceforth.\\n2.4 Training\\nWe jointly train the retriever and generator components without any direct supervision on what\\ndocumentshouldberetrieved. Givenafine-tuningtrainingcorpusofinput/outputpairs(x ,y ),we\\nj j\\n3\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 3, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='(cid:80)\\nminimizethenegativemarginallog-likelihoodofeachtarget, −logp(y |x )usingstochastic\\nj j j\\ngradientdescentwithAdam[28]. UpdatingthedocumentencoderBERT duringtrainingiscostlyas\\nd\\nitrequiresthedocumentindextobeperiodicallyupdatedasREALMdoesduringpre-training[20].\\nWe do not find this step necessary for strong performance, and keep the document encoder (and\\nindex)fixed,onlyfine-tuningthequeryencoderBERT andtheBARTgenerator.\\nq\\n2.5 Decoding\\nAttesttime,RAG-SequenceandRAG-Tokenrequiredifferentwaystoapproximateargmax p(y|x).\\ny\\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p(cid:48)(y |x,y ) = (cid:80) p (z |x)p (y |x,z ,y ) To\\nθ i 1:i−1 z∈top-k(p(·|x)) η i θ i i 1:i−1\\ndecode,wecanplugp(cid:48)(y |x,y )intoastandardbeamdecoder.\\nθ i 1:i−1\\nRAG-Sequence ForRAG-Sequence,thelikelihoodp(y|x)doesnotbreakintoaconventionalper-\\ntokenlikelihood,hencewecannotsolveitwithasinglebeamsearch. Instead,werunbeamsearchfor\\neachdocumentz,scoringeachhypothesisusingp (y |x,z,y ). Thisyieldsasetofhypotheses\\nθ i 1:i−1\\nY,someofwhichmaynothaveappearedinthebeamsofalldocuments. Toestimatetheprobability\\nof an hypothesis y we run an additional forward pass for each document z for which y does not\\nappearinthebeam,multiplygeneratorprobabilitywithp (z|x)andthensumtheprobabilitiesacross\\nη\\nbeamsforthemarginals. Werefertothisdecodingprocedureas“ThoroughDecoding.” Forlonger\\noutputsequences,|Y|canbecomelarge,requiringmanyforwardpasses. Formoreefficientdecoding,\\nwecanmakeafurtherapproximationthatp (y|x,z )≈0whereywasnotgeneratedduringbeam\\nθ i\\nsearchfromx,z . ThisavoidstheneedtorunadditionalforwardpassesoncethecandidatesetY has\\ni\\nbeengenerated. Werefertothisdecodingprocedureas“FastDecoding.”\\n3 Experiments\\nWeexperimentwithRAGinawiderangeofknowledge-intensivetasks. Forallexperiments,weuse\\nasingleWikipediadumpforournon-parametricknowledgesource. FollowingLeeetal.[31]and\\nKarpukhinetal.[26],weusetheDecember2018dump. EachWikipediaarticleissplitintodisjoint\\n100-wordchunks,tomakeatotalof21Mdocuments. Weusethedocumentencodertocomputean\\nembeddingforeachdocument,andbuildasingleMIPSindexusingFAISS[23]withaHierarchical\\nNavigableSmallWorldapproximationforfastretrieval[37]. Duringtraining,weretrievethetop\\nkdocumentsforeachquery. Weconsiderk ∈{5,10}fortrainingandsetkfortesttimeusingdev\\ndata. Wenowdiscussexperimentaldetailsforeachtask.\\n3.1 Open-domainQuestionAnswering\\nOpen-domainquestionanswering(QA)isanimportantreal-worldapplicationandcommontestbed\\nforknowledge-intensivetasks[20]. Wetreatquestionsandanswersasinput-outputtextpairs(x,y)\\nandtrainRAGbydirectlyminimizingthenegativelog-likelihoodofanswers. WecompareRAGto\\nthepopularextractiveQAparadigm[5,7,31,26],whereanswersareextractedspansfromretrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to “Closed-Book\\nQA”approaches[52],which,likeRAG,generateanswers,butwhichdonotexploitretrieval,instead\\nrelyingpurelyonparametricknowledge.Weconsiderfourpopularopen-domainQAdatasets:Natural\\nQuestions(NQ)[29],TriviaQA(TQA)[24]. WebQuestions(WQ)[3]andCuratedTrec(CT)[2]. As\\nCTandWQaresmall,wefollowDPR[26]byinitializingCTandWQmodelswithourNQRAG\\nmodel. Weusethesametrain/dev/testsplitsaspriorwork[31,26]andreportExactMatch(EM)\\nscores. ForTQA,tocomparewithT5[52],wealsoevaluateontheTQAWikitestset.\\n3.2 AbstractiveQuestionAnswering\\nRAGmodelscangobeyondsimpleextractiveQAandanswerquestionswithfree-form,abstractive\\ntextgeneration. TotestRAG’snaturallanguagegeneration(NLG)inaknowledge-intensivesetting,\\nwe use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrievedpassages. Wedonotusethesuppliedpassages,onlythequestionsandanswers,totreat\\n4\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 4, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='MSMARCOasanopen-domainabstractiveQAtask. MSMARCOhassomequestionsthatcannotbe\\nansweredinawaythatmatchesthereferenceanswerwithoutaccesstothegoldpassages,suchas\\n“WhatistheweatherinVolcano,CA?”soperformancewillbelowerwithoutusinggoldpassages.\\nWealsonotethatsomeMSMARCOquestionscannotbeansweredusingWikipediaalone. Here,\\nRAGcanrelyonparametricknowledgetogeneratereasonableresponses.\\n3.3 JeopardyQuestionGeneration\\nToevaluateRAG’sgenerationabilitiesinanon-QAsetting,westudyopen-domainquestiongen-\\neration. Ratherthanusequestionsfromstandardopen-domainQAtasks,whichtypicallyconsist\\nofshort,simplequestions,weproposethemoredemandingtaskofgeneratingJeopardyquestions.\\nJeopardyisanunusualformatthatconsistsoftryingtoguessanentityfromafactaboutthatentity.\\nForexample,“TheWorldCup”istheanswertothequestion“In1986Mexicoscoredasthefirst\\ncountry to host this international sports competition twice.” As Jeopardy questions are precise,\\nfactualstatements,generatingJeopardyquestionsconditionedontheiranswerentitiesconstitutesa\\nchallengingknowledge-intensivegenerationtask.\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\\nthisisanewtask,wetrainaBARTmodelforcomparison. Following[67],weevaluateusingthe\\nSQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandardmetrics. Wealsoperformtwohumanevaluations,onetoassessgenerationfactuality,and\\noneforspecificity.Wedefinefactualityaswhetherastatementcanbecorroboratedbytrustedexternal\\nsources,andspecificityashighmutualdependencebetweentheinputandoutput[33]. Wefollow\\nbestpracticeandusepairwisecomparativeevaluation[34]. Evaluatorsareshownananswerandtwo\\ngeneratedquestions,onefromBARTandonefromRAG.Theyarethenaskedtopickoneoffour\\noptions—quuestionAisbetter,questionBisbetter,botharegood,orneitherisgood.\\n3.4 FactVerification\\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by\\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhethertheclaimistrue,false,orunverifiablefromWikipediaalone. FEVERisaretrievalproblem\\ncoupledwithanchallengingentailmentreasoningtask. Italsoprovidesanappropriatetestbedfor\\nexploringtheRAGmodels’abilitytohandleclassificationratherthangeneration. WemapFEVER\\nclasslabels(supports, refutes, ornotenoughinfo)tosingleoutputtokensanddirectlytrainwith\\nclaim-classpairs. Crucially,unlikemostotherapproachestoFEVER,wedonotusesupervisionon\\nretrievedevidence. Inmanyreal-worldapplications,retrievalsupervisionsignalsaren’tavailable,and\\nmodelsthatdonotrequiresuchsupervisionwillbeapplicabletoawiderrangeoftasks. Weexplore\\ntwovariants: thestandard3-wayclassificationtask(supports/refutes/notenoughinfo)andthe2-way\\n(supports/refutes)taskstudiedinThorneandVlachos[57]. Inbothcaseswereportlabelaccuracy.\\n4 Results\\n4.1 Open-domainQuestionAnswering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks,RAGsetsanewstateoftheart(onlyontheT5-comparablesplitforTQA).RAGcombines\\nthegenerationflexibilityofthe“closed-book”(parametriconly)approachesandtheperformanceof\\n\"open-book\"retrieval-basedapproaches. UnlikeREALMandT5+SSM,RAGenjoysstrongresults\\nwithoutexpensive,specialized“salientspanmasking”pre-training[20]. ItisworthnotingthatRAG’s\\nretrieverisinitializedusingDPR’sretriever,whichusesretrievalsupervisiononNaturalQuestions\\nandTriviaQA.RAGcomparesfavourablytotheDPRQAsystem,whichusesaBERT-based“cross-\\nencoder”tore-rankdocuments,alongwithanextractivereader. RAGdemonstratesthatneithera\\nre-rankernorextractivereaderisnecessaryforstate-of-the-artperformance.\\nThereareseveraladvantagestogeneratinganswersevenwhenitispossibletoextractthem. Docu-\\nmentswithcluesabouttheanswerbutdonotcontaintheanswerverbatimcanstillcontributetowards\\nacorrectanswerbeinggenerated,whichisnotpossiblewithstandardextractiveapproaches,leading\\n5\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 5, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Table1:Open-DomainQATestScores.ForTQA, Table2:GenerationandclassificationTestScores.\\nleft column uses the standard test set for Open- MS-MARCOSotAis[4],FEVER-3is[68]and\\nDomain QA, right column uses the TQA-Wiki FEVER-2 is [57] *Uses gold context/evidence.\\ntestset. SeeAppendixDforfurtherdetails. Bestmodelwithoutgoldaccessunderlined.\\nModel NQ TQA WQ CT\\nModel Jeopardy MSMARCO FVR3 FVR2\\nClosed T5-11B[52] 34.5 - /50.1 37.4 -\\nB-1 QB-1 R-L B-1 LabelAcc.\\nBook T5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nSotA - - 49.8* 49.9* 76.8 92.2*\\nOpen REALM[20] 40.4 - / - 40.7 46.8\\nBook DPR[26] 41.5 57.9/ - 41.1 50.6 BART 15.1 19.7 38.2 41.6 64.0 81.1\\nRAG-Token 44.1 55.2/66.1 45.5 50.0 RAG-Tok. 17.3 22.2 40.1 41.5\\n72.5 89.5\\nRAG-Seq. 44.5 56.8/68.0 45.2 52.2 RAG-Seq. 14.7 21.4 40.8 44.2\\ntomoreeffectivemarginalizationoverdocuments. Furthermore,RAGcangeneratecorrectanswers\\nevenwhenthecorrectanswerisnotinanyretrieveddocument,achieving11.8%accuracyinsuch\\ncasesforNQ,whereanextractivemodelwouldscore0%.\\n4.2 AbstractiveQuestionAnswering\\nAsshowninTable2,RAG-SequenceoutperformsBARTonOpenMS-MARCONLGby2.6Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressivegiventhat(i)thosemodelsaccessgoldpassageswithspecificinformationrequiredto\\ngeneratethereferenceanswer,(ii)manyquestionsareunanswerablewithoutthegoldpassages,and\\n(iii)notallquestionsareanswerablefromWikipediaalone. Table3showssomegeneratedanswers\\nfromourmodels. Qualitatively,wefindthatRAGmodelshallucinatelessandgeneratefactually\\ncorrecttextmoreoftenthanBART.Later,wealsoshowthatRAGgenerationsaremorediversethan\\nBARTgenerations(see§4.5).\\n4.3 JeopardyQuestionGeneration\\nTable2showsthatRAG-TokenperformsbetterthanRAG-SequenceonJeopardyquestiongeneration,\\nwithbothmodelsoutperformingBARTonQ-BLEU-1. 4showshumanevaluationresults,over452\\npairsofgenerationsfromBARTandRAG-Token. EvaluatorsindicatedthatBARTwasmorefactual\\nthanRAGinonly7.1%ofcases,whileRAGwasmorefactualin42.7%ofcases,andbothRAGand\\nBARTwerefactualinafurther17%ofcases,clearlydemonstratingtheeffectivenessofRAGon\\nthetaskoverastate-of-the-artgenerationmodel. EvaluatorsalsofindRAGgenerationstobemore\\nspecificbyalargemargin. Table3showstypicalgenerationsfromeachmodel.\\nJeopardyquestionsoftencontaintwoseparatepiecesofinformation,andRAG-Tokenmayperform\\nbestbecauseitcangenerateresponsesthatcombinecontentfromseveraldocuments. Figure2shows\\nanexample. Whengenerating“Sun”, theposteriorishighfordocument2whichmentions“The\\nSun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is\\ngenerated. Intriguingly,afterthefirsttokenofeachbookisgenerated,thedocumentposteriorflattens.\\nThisobservationsuggeststhatthegeneratorcancompletethetitleswithoutdependingonspecific\\ndocuments. Inotherwords,themodel’sparametricknowledgeissufficienttocompletethetitles. We\\nfindevidenceforthishypothesisbyfeedingtheBART-onlybaselinewiththepartialdecoding\"The\\nSun. BARTcompletesthegeneration\"TheSunAlsoRises\"isanovelbythisauthorof\"TheSun\\nAlsoRises\"indicatingthetitle\"TheSunAlsoRises\"isstoredinBART’sparameters. Similarly,\\nBART will complete the partial decoding \"TheSunAlsoRises\"isanovelbythisauthorof\"A\\nwith\"TheSunAlsoRises\"isanovelbythisauthorof\"AFarewelltoArms\". Thisexampleshows\\nhowparametricandnon-parametricmemoriesworktogether—thenon-parametriccomponenthelps\\ntoguidethegeneration,drawingoutspecificknowledgestoredintheparametricmemory.\\n4.4 FactVerification\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of\\nstate-of-the-artmodels,whicharecomplexpipelinesystemswithdomain-specificarchitecturesand\\nsubstantialengineering,trainedusingintermediateretrievalsupervision,whichRAGdoesnotrequire.\\n6\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 6, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Document1:hisworksareconsideredclassicsofAmerican Doc1\\nliterature...Hiswartimeexperiencesformedthebasisforhisnovel Doc2\\n”AFarewelltoArms”(1929)...\\nDoc3\\nDocument2:...artistsofthe1920s”LostGeneration”expatriate\\nDoc4\\ncommunity.Hisdebutnovel,”TheSunAlsoRises”,waspublished\\nin1926. Doc5\\nB\\nOS ” The Sun Also R ises ” is a novelby this authorof ” A Fare wellto\\nAr\\nms”\\nFigure2: RAG-Tokendocumentposteriorp(z |x,y ,y )foreachgeneratedtokenforinput“Hem-\\ni i −i\\ningway\"forJeopardygenerationwith5retrieveddocuments. Theposteriorfordocument1ishigh\\nwhengenerating“AFarewelltoArms\"andfordocument2whengenerating“TheSunAlsoRises\".\\nTable3: Examplesfromgenerationtasks. RAGmodelsgeneratemorespecificandfactuallyaccurate\\nresponses. ‘?’ indicatesfactuallyincorrectresponses,*indicatespartiallycorrectresponses.\\nTask Input Model Generation\\nBART ?Themiddleearisthepartoftheearbetweenthemiddleearandthenose.\\ndefinemiddle\\nRAG-T Themiddleearistheportionoftheearinternaltotheeardrum.\\near\\nMS- RAG-S Themiddleearincludesthetympaniccavityandthethreeossicles.\\nMARCO\\nwhatcurrency BART ThecurrencyneededinScotlandisPoundsterling.\\nneededin RAG-T PoundisthecurrencyneededinScotland.\\nscotland RAG-S ThecurrencyneededinScotlandisthepoundsterling.\\nBART ?ThisstatehasthelargestnumberofcountiesintheU.S.\\nJeopardy Washington RAG-T It’stheonlyU.S.statenamedforaU.S.president\\nQuestion RAG-S It’sthestatewhereyou’llfindMountRainierNationalPark\\nGener\\nBART *ThisepicpoembyDanteisdividedinto3parts:theInferno,thePurgatorio&thePurgatorio\\n-ation TheDivine\\nRAG-T Dante’s\"Inferno\"isthefirstpartofthisepicpoem\\nComedy\\nRAG-S This14thcenturyworkisdividedinto3sections:\"Inferno\",\"Purgatorio\"&\"Paradiso\"\\nFor2-wayclassification,wecompareagainstThorneandVlachos[57],whotrainRoBERTa[35]\\ntoclassifytheclaimastrueorfalsegiventhegoldevidencesentence. RAGachievesanaccuracy\\nwithin2.7%ofthismodel,despitebeingsuppliedwithonlytheclaimandretrievingitsownevidence.\\nWealsoanalyzewhetherdocumentsretrievedbyRAGcorrespondtodocumentsannotatedasgold\\nevidenceinFEVER.Wecalculatetheoverlapinarticletitlesbetweenthetopkdocumentsretrieved\\nbyRAGandgoldevidenceannotations. Wefindthatthetopretrieveddocumentisfromagoldarticle\\nin71%ofcases,andagoldarticleispresentinthetop10retrievedarticlesin90%ofcases.\\n4.5 AdditionalResults\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and specific than\\nBARTforJeopardyquestiongeneration. Followingrecentworkondiversity-promotingdecoding\\n[33,59,39],wealsoinvestigategenerationdiversitybycalculatingtheratioofdistinctngramsto\\ntotalngramsgeneratedbydifferentmodels. Table5showsthatRAG-Sequence’sgenerationsare\\nmorediversethanRAG-Token’s,andbotharesignificantlymorediversethanBARTwithoutneeding\\nanydiversity-promotingdecoding.\\nRetrievalAblations AkeyfeatureofRAGislearningtoretrieverelevantinformationforthetask.\\nToassesstheeffectivenessoftheretrievalmechanism,werunablationswherewefreezetheretriever\\nduringtraining. AsshowninTable6,learnedretrievalimprovesresultsforalltasks.\\nWecompareRAG’sdenseretrievertoawordoverlap-basedBM25retriever[53]. Here,wereplace\\nRAG’sretrieverwithafixedBM25system,anduseBM25retrievalscoresaslogitswhencalculating\\np(z|x).Table6showstheresults.ForFEVER,BM25performsbest,perhapssinceFEVERclaimsare\\nheavilyentity-centricandthuswell-suitedforwordoverlap-basedretrieval. Differentiableretrieval\\nimprovesresultsonallothertasks,especiallyforOpen-DomainQA,whereitiscrucial.\\nIndexhot-swapping Anadvantageofnon-parametricmemorymodelslikeRAGisthatknowledge\\ncanbeeasilyupdatedattesttime. Parametric-onlymodelslikeT5orBARTneedfurthertrainingto\\nupdatetheirbehaviorastheworldchanges. Todemonstrate,webuildanindexusingtheDrQA[5]\\nWikipediadumpfromDecember2016andcompareoutputsfromRAGusingthisindextothenewer\\nindexfromourmainresults(December2018). Wepreparealistof82worldleaderswhohadchanged\\n7\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 7, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Table 4: Human assessments for the Jeopardy Table 5: Ratio of distinct to total tri-grams for\\nQuestionGenerationTask. generationtasks.\\nFactuality Specificity\\nMSMARCO JeopardyQGen\\nBARTbetter 7.1% 16.8%\\nGold 89.6% 90.0%\\nRAGbetter 42.7% 37.4%\\nBART 70.7% 32.4%\\nBothgood 11.7% 11.8%\\nRAG-Token 77.8% 46.8%\\nBothpoor 17.7% 6.9%\\nRAG-Seq. 83.5% 53.8%\\nNomajority 20.8% 20.1%\\nTable6:Ablationsonthedevset.AsFEVERisaclassificationtask,bothRAGmodelsareequivalent.\\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\\nExactMatch B-1 QB-1 R-L B-1 LabelAccuracy\\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4\\n75.1 91.6\\nRAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4\\n72.9 89.4\\nRAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4\\n74.5 90.6\\nRAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5\\nbetweenthesedatesanduseatemplate“Whois{position}?” (e.g. “WhoisthePresidentofPeru?”)\\ntoqueryourNQRAGmodelwitheachindex. RAGanswers70%correctlyusingthe2016indexfor\\n2016worldleadersand68%usingthe2018indexfor2018worldleaders. Accuracywithmismatched\\nindicesislow(12%withthe2018indexand2016leaders,4%withthe2016indexand2018leaders).\\nThisshowswecanupdateRAG’sworldknowledgebysimplyreplacingitsnon-parametricmemory.\\nEffect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\\ndocuments,andwedonotobservesignificantdifferencesinperformancebetweenthem. Wehavethe\\nflexibilitytoadjustthenumberofretrieveddocumentsattesttime,whichcanaffectperformanceand\\nruntime. Figure3(left)showsthatretrievingmoredocumentsattesttimemonotonicallyimproves\\nOpen-domainQAresultsforRAG-Sequence,butperformancepeaksforRAG-Tokenat10retrieved\\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-TokenattheexpenseofBleu-1,buttheeffectislesspronouncedforRAG-Sequence.\\n44\\n43\\n42\\n41\\n40\\n39\\n10 20 30 40 50\\nKRetrievedDocs\\nhctaMtcaxEQN\\n80\\n70\\n60\\n50 RAG-Tok\\nRAG-Seq 40\\n10 20 30 40 50\\nKRetrievedDocs\\nK@llaceRrewsnAQN\\n56\\n54\\n52 RAG-Tok\\nRAG-Seq 50 FixedDPR\\nBM25 48\\n10 20 30 40 50\\nKRetrievedDocs\\nerocsL-eguoR/1-uelB\\nRAG-TokR-L\\nRAG-TokB-1\\nRAG-SeqR-L\\nRAG-SeqB-1\\nFigure3: Left: NQperformanceasmoredocumentsareretrieved. Center: Retrievalrecallperfor-\\nmanceinNQ.Right: MS-MARCOBleu-1andRouge-Lasmoredocumentsareretrieved.\\n5 RelatedWork\\nSingle-TaskRetrieval Priorworkhasshownthatretrievalimprovesperformanceacrossavarietyof\\nNLPtaskswhenconsideredinisolation. Suchtasksincludeopen-domainquestionanswering[5,29],\\nfact checking [56], fact completion [48], long-form question answering [12], Wikipedia article\\ngeneration [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our\\nworkunifiesprevioussuccessesinincorporatingretrievalintoindividualtasks,showingthatasingle\\nretrieval-basedarchitectureiscapableofachievingstrongperformanceacrossseveraltasks.\\n8\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 8, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='General-PurposeArchitecturesforNLP Priorworkongeneral-purposearchitecturesforNLP\\ntaskshasshowngreatsuccesswithouttheuseofretrieval. Asingle, pre-trainedlanguagemodel\\nhasbeenshowntoachievestrongperformanceonvariousclassificationtasksintheGLUEbench-\\nmarks[60,61]afterfine-tuning[49,8].GPT-2[50]latershowedthatasingle,left-to-right,pre-trained\\nlanguagemodelcouldachievestrongperformanceacrossbothdiscriminativeandgenerativetasks.\\nForfurtherimprovement,BART[32]andT5[51,52]proposeasingle,pre-trainedencoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nandgenerativetasks. Ourworkaimstoexpandthespaceofpossibletaskswithasingle, unified\\narchitecture,bylearningaretrievalmoduletoaugmentpre-trained,generativelanguagemodels.\\nLearned Retrieval There is significant work on learning to retrieve documents in information\\nretrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\\nworkoptimizestheretrievalmoduletoaidinaspecific,downstreamtasksuchasquestionanswering,\\nusingsearch[46],reinforcementlearning[6,63,62],oralatentvariableapproach[31,20]asinour\\nwork. Thesesuccessesleveragedifferentretrieval-basedarchitecturesandoptimizationtechniquesto\\nachievestrongperformanceonasingletask,whileweshowthatasingleretrieval-basedarchitecture\\ncanbefine-tunedforstrongperformanceonavarietyoftasks.\\nMemory-basedArchitectures Ourdocumentindexcanbeseenasalargeexternalmemoryfor\\nneuralnetworkstoattendto,analogoustomemorynetworks[64,55]. Concurrentwork[14]learns\\ntoretrieveatrainedembeddingforeachentityintheinput,ratherthantoretrieverawtextasinour\\nwork. Otherworkimprovestheabilityofdialogmodelstogeneratefactualtextbyattendingover\\nfactembeddings[15,13]. Akeyfeatureofourmemoryisthatitiscomprisedofrawtextrather\\ndistributedrepresentations,whichmakesthememoryboth(i)human-readable,lendingaformof\\ninterpretabilitytoourmodel,and(ii)human-writable,enablingustodynamicallyupdatethemodel’s\\nmemorybyeditingthedocumentindex. Thisapproachhasalsobeenusedinknowledge-intensive\\ndialog,wheregeneratorshavebeenconditionedonretrievedtextdirectly,albeitobtainedviaTF-IDF\\nratherthanend-to-endlearntretrieval[9].\\nRetrieve-and-Editapproaches Ourmethodsharessomesimilaritieswithretrieve-and-editstyle\\napproaches,whereasimilartraininginput-outputpairisretrievedforagiveninput,andthenedited\\ntoprovideafinaloutput. Theseapproacheshaveprovedsuccessfulinanumberofdomainsincluding\\nMachineTranslation [18,22]andSemanticParsing[21].Ourapproachdoeshaveseveraldifferences,\\nincludinglessofemphasisonlightlyeditingaretrieveditem,butonaggregatingcontentfromseveral\\npiecesofretrievedcontent,aswellaslearninglatentretrieval,andretrievingevidencedocuments\\nratherthanrelatedtrainingpairs. Thissaid,RAGtechniquesmayworkwellinthesesettings,and\\ncouldrepresentpromisingfuturework.\\n6 Discussion\\nInthiswork,wepresentedhybridgenerationmodelswithaccesstoparametricandnon-parametric\\nmemory. WeshowedthatourRAGmodelsobtainstateoftheartresultsonopen-domainQA.We\\nfoundthatpeoplepreferRAG’sgenerationoverpurelyparametricBART,findingRAGmorefactual\\nandspecific. Weconductedanthoroughinvestigationofthelearnedretrievalcomponent,validating\\nitseffectiveness,andweillustratedhowtheretrievalindexcanbehot-swappedtoupdatethemodel\\nwithoutrequiringanyretraining.Infuturework,itmaybefruitfultoinvestigateifthetwocomponents\\ncanbejointlypre-trainedfromscratch,eitherwithadenoisingobjectivesimilartoBARTorsome\\nanotherobjective. Ourworkopensupnewresearchdirectionsonhowparametricandnon-parametric\\nmemoriesinteractandhowtomosteffectivelycombinethem,showingpromiseinbeingappliedtoa\\nwidevarietyofNLPtasks.\\n9\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 9, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='BroaderImpact\\nThis work offers several positive societal benefits over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less\\nwithgenerationsthataremorefactual,andoffersmorecontrolandinterpretability. RAGcouldbe\\nemployedinawidevarietyofscenarioswithdirectbenefittosociety,forexamplebyendowingit\\nwithamedicalindexandaskingitopen-domainquestionsonthattopic,orbyhelpingpeoplebemore\\neffectiveattheirjobs.\\nWiththeseadvantagesalsocomepotentialdownsides:Wikipedia,oranypotentialexternalknowledge\\nsource,willprobablyneverbeentirelyfactualandcompletelydevoidofbias. SinceRAGcanbe\\nemployedasalanguagemodel,similarconcernsasforGPT-2[50]arevalidhere,althougharguably\\ntoalesserextent,includingthatitmightbeusedtogenerateabuse,fakedormisleadingcontentin\\nthenewsoronsocialmedia;toimpersonateothers;ortoautomatetheproductionofspam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncomingdecades[16]. Inordertomitigatetheserisks,AIsystemscouldbeemployedtofightagainst\\nmisleadingcontentandautomatedspam/phishing.\\nAcknowledgments\\nTheauthorswouldliketothankthereviewersfortheirthoughtfulandconstructivefeedbackonthis\\npaper,aswellasHuggingFacefortheirhelpinopen-sourcingcodetorunRAGmodels. Theauthors\\nwouldalsoliketothankKyunghyunChoandSewonMinforproductivediscussionsandadvice. EP\\nthankssupportsfromtheNSFGraduateResearchFellowship. PLissupportedbytheFAIRPhD\\nprogram.\\nReferences\\n[1] PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,Rangan\\nMajumder,AndrewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n//arxiv.org/abs/1611.09268. arXiv: 1611.09268.\\n[2] PetrBaudišandJanŠedivy`. Modelingofthequestionansweringtaskintheyodaqasystem. In\\nInternationalConferenceoftheCross-LanguageEvaluationForumforEuropeanLanguages,\\npages222–228.Springer,2015. URLhttps://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20.\\n[3] JonathanBerant,AndrewChou,RoyFrostig,andPercyLiang. SemanticParsingonFreebase\\nfromQuestion-AnswerPairs. InProceedingsofthe2013ConferenceonEmpiricalMethods\\ninNaturalLanguageProcessing,pages1533–1544,Seattle,Washington,USA,October2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160.\\n[4] BinBi,ChenliangLi,ChenWu,MingYan,andWeiWang. Palm: Pre-traininganautoencod-\\ning&autoregressivelanguagemodelforcontext-conditionedgeneration. ArXiv,abs/2004.07159,\\n2020. URLhttps://arxiv.org/abs/2004.07159.\\n[5] DanqiChen,AdamFisch,JasonWeston,andAntoineBordes. ReadingWikipediatoAnswer\\nOpen-DomainQuestions. InProceedingsofthe55thAnnualMeetingoftheAssociationfor\\nComputationalLinguistics(Volume1: LongPapers),pages1870–1879,Vancouver,Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\\nhttps://www.aclweb.org/anthology/P17-1171.\\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathanBerant. Coarse-to-finequestionansweringforlongdocuments. InProceedingsofthe\\n55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),\\npages209–220,Vancouver,Canada,July2017.AssociationforComputationalLinguistics. doi:\\n10.18653/v1/P17-1020. URLhttps://www.aclweb.org/anthology/P17-1020.\\n10\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 10, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[7] ChristopherClarkandMattGardner. SimpleandEffectiveMulti-ParagraphReadingCompre-\\nhension. arXiv:1710.10723[cs],October2017. URLhttp://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingof\\nDeepBidirectionalTransformersforLanguageUnderstanding. InProceedingsofthe2019Con-\\nferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human\\nLanguageTechnologies,Volume1(LongandShortPapers),pages4171–4186,Minneapolis,\\nMinnesota,June2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/N19-1423.\\nURLhttps://www.aclweb.org/anthology/N19-1423.\\n[9] EmilyDinan,StephenRoller,KurtShuster,AngelaFan,MichaelAuli,andJasonWeston. Wiz-\\nardofwikipedia: Knowledge-poweredconversationalagents. InInternationalConferenceon\\nLearningRepresentations,2019. URLhttps://openreview.net/forum?id=r1l73iRqKm.\\n[10] MatthewDunn,LeventSagun,MikeHiggins,V.UgurGuney,VolkanCirik,andKyunghyun\\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:\\n1704.05179.\\n[11] AngelaFan,MikeLewis,andYannDauphin. Hierarchicalneuralstorygeneration. InProceed-\\ningsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:\\nLongPapers),pages889–898,Melbourne,Australia,July2018.AssociationforComputational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082.\\n[12] AngelaFan,YacineJernite,EthanPerez,DavidGrangier,JasonWeston,andMichaelAuli.ELI5:\\nLongformquestionanswering. InProceedingsofthe57thAnnualMeetingoftheAssociation\\nforComputationalLinguistics,pages3558–3567,Florence,Italy,July2019.Associationfor\\nComputationalLinguistics. doi: 10.18653/v1/P19-1346. URLhttps://www.aclweb.org/\\nanthology/P19-1346.\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwithKNN-basedcompositememory, 2020. URLhttps://openreview.net/forum?id=\\nH1gx1CNKPH.\\n[14] ThibaultFévry,LivioBaldiniSoares,NicholasFitzGerald,EunsolChoi,andTomKwiatkowski.\\nEntitiesasexperts: Sparsememoryaccesswithentitysupervision. ArXiv, abs/2004.07202,\\n2020. URLhttps://arxiv.org/abs/2004.07202.\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConferenceonArtificialIntelligence,2018.URLhttps://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710.\\n[16] KatjaGrace,JohnSalvatier,AllanDafoe,BaobaoZhang,andOwainEvans. WhenwillAI\\nexceedhumanperformance? evidencefromAIexperts. CoRR,abs/1705.08807,2017. URL\\nhttp://arxiv.org/abs/1705.08807.\\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In AAAI Conference on Artificial Intelligence, 2018. URL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachinetranslation. In 32ndAAAIConferenceonArtificialIntelligence, AAAI 2018, 32nd\\nAAAIConferenceonArtificialIntelligence,AAAI2018,pages5133–5140.AAAIpress,2018.\\n32ndAAAIConferenceonArtificialIntelligence,AAAI2018;Conferencedate: 02-02-2018\\nThrough07-02-2018.\\n[19] KelvinGuu,TatsunoriB.Hashimoto,YonatanOren,andPercyLiang. Generatingsentencesby\\neditingprototypes. TransactionsoftheAssociationforComputationalLinguistics,6:437–450,\\n2018. doi: 10.1162/tacl_a_00030. URLhttps://www.aclweb.org/anthology/Q18-1031.\\n11\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 11, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmentedlanguagemodelpre-training. ArXiv,abs/2002.08909,2020. URLhttps:\\n//arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems 31, pages 10052–\\n10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] NabilHossain,MarjanGhazvininejad,andLukeZettlemoyer. Simpleandeffectiveretrieve-\\nedit-reranktextgeneration. InProceedingsofthe58thAnnualMeetingoftheAssociationfor\\nComputationalLinguistics,pages2532–2538,Online,July2020.AssociationforComputa-\\ntionalLinguistics. doi: 10.18653/v1/2020.acl-main.228. URLhttps://www.aclweb.org/\\nanthology/2020.acl-main.228.\\n[23] JeffJohnson,MatthijsDouze,andHervéJégou. Billion-scalesimilaritysearchwithgpus. arXiv\\npreprintarXiv:1702.08734,2017. URLhttps://arxiv.org/abs/1702.08734.\\n[24] MandarJoshi,EunsolChoi,DanielWeld,andLukeZettlemoyer. TriviaQA:ALargeScale\\nDistantlySupervisedChallengeDatasetforReadingComprehension. InProceedingsofthe\\n55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),\\npages1601–1611,Vancouver,Canada,July2017.AssociationforComputationalLinguistics.\\ndoi: 10.18653/v1/P17-1147. URLhttps://www.aclweb.org/anthology/P17-1147.\\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\\naugmented recurrent nets. In Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1, NIPS’15, page 190–198, Cam-\\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/\\n5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.\\n[26] VladimirKarpukhin,BarlasOguz,SewonMin,LedellWu,SergeyEdunov,DanqiChen,and\\nWen-tauYih. Densepassageretrievalforopen-domainquestionanswering. arXivpreprint\\narXiv:2004.04906,2020. URLhttps://arxiv.org/abs/2004.04906.\\n[27] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis.Generaliza-\\ntionthroughmemorization: Nearestneighborlanguagemodels. InInternationalConferenceon\\nLearningRepresentations,2020. URLhttps://openreview.net/forum?id=HklBjCEKvH.\\n[28] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InYoshua\\nBengioandYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,\\nICLR2015,SanDiego,CA,USA,May7-9,2015,ConferenceTrackProceedings,2015. URL\\nhttp://arxiv.org/abs/1412.6980.\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\\ntion Answering Research. Transactions of the Association of Computational Lin-\\nguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf.\\n[30] GuillaumeLample,AlexandreSablayrolles,Marc’AurelioRanzato,LudovicDenoyer,and\\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformationProcessingSystems32,pages8548–8559.CurranAssociates,Inc.,2019. URLhttp:\\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf.\\n[31] KentonLee,Ming-WeiChang,andKristinaToutanova. Latentretrievalforweaklysupervised\\nopendomainquestionanswering. InProceedingsofthe57thAnnualMeetingoftheAssociation\\n12\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 12, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='forComputationalLinguistics,pages6086–6096,Florence,Italy,July2019.Associationfor\\nComputationalLinguistics. doi: 10.18653/v1/P19-1612. URLhttps://www.aclweb.org/\\nanthology/P19-1612.\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmerLevy,VeselinStoyanov,andLukeZettlemoyer. BART:Denoisingsequence-to-sequence\\npre-trainingfornaturallanguagegeneration,translation,andcomprehension. arXivpreprint\\narXiv:1910.13461,2019. URLhttps://arxiv.org/abs/1910.13461.\\n[33] JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,andBillDolan. Adiversity-promoting\\nobjectivefunctionforneuralconversationmodels. InProceedingsofthe2016Conferenceofthe\\nNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage\\nTechnologies,pages110–119,SanDiego,California,June2016.AssociationforComputational\\nLinguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014.\\n[34] MargaretLi, JasonWeston, andStephenRoller. Acute-eval: Improveddialogueevaluation\\nwith optimizedquestions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087.\\n[35] HairongLiu,MingboMa,LiangHuang,HaoXiong,andZhongjunHe. Robustneuralmachine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual\\nMeetingoftheAssociationforComputationalLinguistics,pages3044–3049,Florence,Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291.\\n[36] PeterJ.Liu*,MohammadSaleh*,EtiennePot,BenGoodrich,RyanSepassi,LukaszKaiser,\\nandNoamShazeer. Generatingwikipediabysummarizinglongsequences. InInternational\\nConferenceonLearningRepresentations,2018. URLhttps://openreview.net/forum?\\nid=Hyg0vbWC-.\\n[37] YuryA.MalkovandD.A.Yashunin. Efficientandrobustapproximatenearestneighborsearch\\nusinghierarchicalnavigablesmallworldgraphs. IEEETransactionsonPatternAnalysisand\\nMachineIntelligence,42:824–836,2016. URLhttps://arxiv.org/abs/1603.09320.\\n[38] GaryMarcus. Thenextdecadeinai: fourstepstowardsrobustartificialintelligence. arXiv\\npreprintarXiv:2002.06177,2020. URLhttps://arxiv.org/abs/2002.06177.\\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\\nverifiability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https:\\n//arxiv.org/abs/1911.03587.\\n[40] PauliusMicikevicius,SharanNarang,JonahAlben,GregoryDiamos,ErichElsen,DavidGarcia,\\nBorisGinsburg,MichaelHouston,OleksiiKuchaiev,GaneshVenkatesh,andHaoWu. Mixed\\nprecisiontraining. InICLR,2018. URLhttps://openreview.net/forum?id=r1gs9JgRZ.\\n[41] NikitaMoghe,SiddharthaArora,SumanBanerjee,andMiteshM.Khapra. Towardsexploit-\\ning background knowledge for building conversation systems. In Proceedings of the 2018\\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2322–2332,Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1255. URLhttps://www.aclweb.org/anthology/D18-1255.\\n[42] PrekshaNemaandMiteshM.Khapra.Towardsabettermetricforevaluatingquestiongeneration\\nsystems. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage\\nProcessing,pages3950–3959,Brussels,Belgium,October-November2018.Associationfor\\nComputationalLinguistics. doi: 10.18653/v1/D18-1429. URLhttps://www.aclweb.org/\\nanthology/D18-1429.\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\\nandLiDeng. MSMARCO:Ahumangeneratedmachinereadingcomprehensiondataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,\\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 13, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='approaches2016co-locatedwiththe30thAnnualConferenceonNeuralInformationProcessing\\nSystems(NIPS2016),Barcelona,Spain,December9,2016,volume1773ofCEURWorkshop\\nProceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf.\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085,2019. URLhttps://arxiv.org/abs/1901.04085.\\n[45] MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,SamGross,NathanNg,DavidGrangier,\\nandMichaelAuli. fairseq: Afast,extensibletoolkitforsequencemodeling. InProceedings\\nofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputational\\nLinguistics(Demonstrations),pages48–53,Minneapolis,Minnesota,June2019.Association\\nfor Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009.\\n[46] EthanPerez,SiddharthKaramcheti,RobFergus,JasonWeston,DouweKiela,andKyunghyun\\nCho. Findinggeneralizableevidencebylearningtoconvinceq&amodels. InProceedings\\nofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\\n2402–2411,HongKong,China,November2019.AssociationforComputationalLinguistics.\\ndoi: 10.18653/v1/D19-1244. URLhttps://www.aclweb.org/anthology/D19-1244.\\n[47] FabioPetroni,TimRocktäschel,SebastianRiedel,PatrickLewis,AntonBakhtin,YuxiangWu,\\nandAlexanderMiller. Languagemodelsasknowledgebases? InProceedingsofthe2019\\nConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternational\\nJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages2463–2473,Hong\\nKong,China,November2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/\\nD19-1250. URLhttps://www.aclweb.org/anthology/D19-1250.\\n[48] FabioPetroni,PatrickLewis,AleksandraPiktus,TimRocktäschel,YuxiangWu,AlexanderH.\\nMiller,andSebastianRiedel. Howcontextaffectslanguagemodels’factualpredictions. In\\nAutomatedKnowledgeBaseConstruction,2020. URLhttps://openreview.net/forum?\\nid=025X0zPfn.\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\\nproving Language Understanding by Generative Pre-Training, 2018. URL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf.\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask learners, 2019. URL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_\\nmodels_are_unsupervised_multitask_learners.pdf.\\n[51] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,\\nYanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunified\\ntext-to-texttransformer. arXive-prints,2019. URLhttps://arxiv.org/abs/1910.10683.\\n[52] AdamRoberts, ColinRaffel, andNoamShazeer. Howmuchknowledgecanyoupackinto\\ntheparametersofalanguagemodel? arXive-prints,2020. URLhttps://arxiv.org/abs/\\n2002.08910.\\n[53] StephenRobertsonandHugoZaragoza. Theprobabilisticrelevanceframework: Bm25and\\nbeyond. Found.TrendsInf.Retr.,3(4):333–389,April2009. ISSN1554-0669. doi: 10.1561/\\n1500000019. URLhttps://doi.org/10.1561/1500000019.\\n[54] IreneSolaiman,MilesBrundage,JackClark,AmandaAskell,ArielHerbert-Voss,JeffWu,Alec\\nRadford,andJian-BingWang. Releasestrategiesandthesocialimpactsoflanguagemodels.\\nArXiv,abs/1908.09203,2019.\\n[55] SainbayarSukhbaatar,ArthurSzlam,JasonWeston,andRobFergus. End-to-endmemorynet-\\nworks.InC.Cortes,N.D.Lawrence,D.D.Lee,M.Sugiyama,andR.Garnett,editors,Advances\\ninNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates,Inc.,2015.\\nURLhttp://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf.\\n14\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 14, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[56] JamesThorne,AndreasVlachos,ChristosChristodoulopoulos,andArpitMittal. FEVER:a\\nlarge-scaledatasetforfactextractionandVERification. InProceedingsofthe2018Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguageTechnologies, Volume1(LongPapers), pages809–819, NewOrleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074.\\n[57] JamesH.ThorneandAndreasVlachos. Avoidingcatastrophicforgettinginmitigatingmodel\\nbiasesinsentence-pairclassificationwithelasticweightconsolidation. ArXiv,abs/2004.14366,\\n2020. URLhttps://arxiv.org/abs/2004.14366.\\n[58] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,\\nŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InI.Guyon,U.V.Luxburg,\\nS.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeural\\nInformationProcessingSystems30,pages5998–6008.CurranAssociates,Inc.,2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun,StefanLee,David\\nCrandall,andDhruvBatra. Diversebeamsearchforimproveddescriptionofcomplexscenes.\\nAAAIConferenceonArtificialIntelligence,2018. URLhttps://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeuralNetworksforNLP,pages353–355,Brussels,Belgium,November2018.Associationfor\\nComputationalLinguistics. doi: 10.18653/v1/W18-5446. URLhttps://www.aclweb.org/\\nanthology/W18-5446.\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF.d\\\\textquotesingleAlché-Buc,E.Fox,andR.Garnett,editors,AdvancesinNeuralInformation\\nProcessing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537.\\n[62] ShuohangWang,MoYu,XiaoxiaoGuo,ZhiguoWang,TimKlinger,WeiZhang,ShiyuChang,\\n3\\nGerryTesauro,BowenZhou,andJingJiang. R : Reinforcedranker-readerforopen-domain\\nquestionanswering. InSheilaA.McIlraithandKilianQ.Weinberger,editors,Proceedingsof\\ntheThirty-SecondAAAIConferenceonArtificialIntelligence,(AAAI-18),the30thinnovative\\nApplicationsofArtificialIntelligence(IAAI-18),andthe8thAAAISymposiumonEducational\\nAdvancesinArtificialIntelligence(EAAI-18), NewOrleans, Louisiana, USA,February2-7,\\n2018, pages 5981–5988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712.\\n[63] ShuohangWang,MoYu,JingJiang,WeiZhang,XiaoxiaoGuo,ShiyuChang,ZhiguoWang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\\nrankinginopen-domainquestionanswering. InICLR,2018. URLhttps://openreview.\\nnet/forum?id=rJl3yM-Ab.\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nandYannLeCun,editors,3rdInternationalConferenceonLearningRepresentations,ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL\\nhttp://arxiv.org/abs/1410.3916.\\n[65] JasonWeston,EmilyDinan,andAlexanderMiller. Retrieveandrefine: Improvedsequence\\ngenerationmodelsfordialogue. InProceedingsofthe2018EMNLPWorkshopSCAI:The2nd\\nInternationalWorkshoponSearch-OrientedConversationalAI,pages87–92,Brussels,Belgium,\\nOctober2018.AssociationforComputationalLinguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713.\\n15\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 15, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[66] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,Anthony\\nMoi, PierricCistac, TimRault, RémiLouf, MorganFuntowicz, JoeDavison, SamShleifer,\\nPatrickvonPlaten,ClaraMa,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,Sylvain\\nGugger,MariamaDrame,QuentinLhoest,andAlexanderM.Rush.Huggingface’stransformers:\\nState-of-the-artnaturallanguageprocessing. ArXiv,abs/1910.03771,2019.\\n[67] ShiyueZhangandMohitBansal. Addressingsemanticdriftinquestiongenerationforsemi-\\nsupervisedquestionanswering. InProceedingsofthe2019ConferenceonEmpiricalMeth-\\nodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNatural\\nLanguage Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253.\\n[68] WanjunZhong,JingjingXu,DuyuTang,ZenanXu,NanDuan,MingZhou,JiahaiWang,and\\nJianYin. Reasoningoversemantic-levelgraphforfactchecking. ArXiv,abs/1909.03745,2019.\\nURLhttps://arxiv.org/abs/1909.03745.\\n16\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 16, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA ImplementationDetails\\nForOpen-domainQAwereporttestnumbersusing15retrieveddocumentsforRAG-Tokenmodels.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThoroughDecodingapproachsinceanswersaregenerallyshort. WeusegreedydecodingforQAas\\nwedidnotfindbeamsearchimprovedresults. ForOpen-MSMarcoandJeopardyquestiongeneration,\\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nandwealsotrainaBART-largemodelasabaseline. Weuseabeamsizeoffour,andusetheFast\\nDecodingapproachforRAG-Sequencemodels,asThoroughDecodingdidnotimproveperformance.\\nB HumanEvaluation\\nFigure4: Annotationinterfaceforhumanevaluationoffactuality. Apop-outfordetailedinstructions\\nandaworkedexampleappearwhenclicking\"viewtoolguide\".\\nFigure 4showsthe userinterface forhuman evaluation. Toavoidanybiases forscreen position,\\nwhichmodelcorrespondedtosentenceAandsentenceBwasrandomlyselectedforeachexample.\\nAnnotatorswereencouragedtoresearchthetopicusingtheinternet,andweregivendetailedinstruc-\\ntionsandworkedexamplesinafullinstructionstab. Weincludedsomegoldsentencesinorderto\\nassesstheaccuracyoftheannotators. Twoannotatorsdidnotperformwellontheseexamplesand\\ntheirannotationswereremovedfromtheresults.\\nC TrainingsetupDetails\\nWetrainallRAGmodelsandBARTbaselinesusingFairseq[45].2 Wetrainwithmixedprecision\\nfloatingpointarithmetic[40],distributingtrainingacross8,32GBNVIDIAV100GPUs,though\\ntrainingandinferencecanberunononeGPU.WefindthatdoingMaximumInnerProductSearch\\nwithFAISSissufficientlyfastonCPU,sowestoredocumentindexvectorsonCPU,requiring∼100\\nGBofCPUmemoryforallofWikipedia. Aftersubmission,WehaveportedourcodetoHuggingFace\\nTransformers[66]3,whichachievesequivalentperformancetothepreviousversionbutisacleaner\\nandeasiertouseimplementation. Thisversionisalsoopen-sourced. Wealsocompressthedocument\\nindexusingFAISS’scompressiontools,reducingtheCPUmemoryrequirementto36GB.Scriptsto\\nrunexperimentswithRAGcanbefoundathttps://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.mdandaninteractivedemoofaRAGmodelcanbefound\\nathttps://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 17, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='D FurtherDetailsonOpen-DomainQA\\nForopen-domainQA,multipleanswerannotationsareoftenavailableforagivenquestion. These\\nanswerannotationsareexploitedbyextractivemodelsduringtrainingastypicallyalltheanswer\\nannotationsareusedtofindmatcheswithindocumentswhenpreparingtrainingdata. ForRAG,we\\nalsomakeuseofmultipleannotationexamplesforNaturalQuestionsandWebQuestionsbytraining\\nthemodelwitheach(q,a)pairseparately,leadingtoasmallincreaseinaccuracy. ForTriviaQA,\\nthereareoftenmanyvalidanswerstoagivenquestion,someofwhicharenotsuitabletrainingtargets,\\nsuchasemojiorspellingvariants. ForTriviaQA,wefilteroutanswercandidatesiftheydonotoccur\\nintop1000documentsforthequery.\\nCuratedTrecpreprocessing TheanswersforCuratedTrecaregivenintheformofregularexpres-\\nsions,whichhasbeensuggestedasareasonwhyitisunsuitableforanswer-generationmodels[20].\\nToovercomethis,weuseapre-processingstepwherewefirstretrievethetop1000documentsfor\\neachquery,andusetheanswerthatmostfrequentlymatchestheregexpatternasthesupervision\\ntarget. Ifnomatchesarefound,weresorttoasimpleheuristic: generateallpossiblepermutationsfor\\neachregex,replacingnon-deterministicsymbolsintheregexnestedtreestructurewithawhitespace.\\nTriviaQAEvaluationsetups Theopen-domainQAcommunitycustomarilyusespublicdevelop-\\nmentdatasetsastestdatasets,astestdataforQAdatasetsisoftenrestrictedanddedicatedtoreading\\ncompehensionpurposes. WereportourresultsusingthedatasetssplitsusedinDPR[26],whichare\\nconsistentwithcommonpracticeinOpen-domainQA.ForTriviaQA,thistestdatasetisthepublic\\nTriviaQAWebDevelopmentsplit. Robertsetal.[52]usedtheTriviaQAofficialWikipediatestset\\ninstead. Févryetal.[14]followthisconventioninordertocomparewithRobertsetal.[52](See\\nappendixof[14]). Wereportresultsonbothtestsetstoenablefaircomparisontobothapproaches.\\nWefindthatourperformanceismuchhigherusingtheofficialWikitestset,ratherthanthemore\\nconventionalopen-domaintestset,whichweattributetotheofficialWikitestsetquestionsbeing\\nsimplertoanswerfromWikipedia.\\nE FurtherDetailsonFEVER\\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and\\nthenclassifyusingtherepresentationofthefinalhiddenstate,beforefinallymarginalizingacross\\ndocumentstoobtaintheclassprobabilities. TheFEVERtasktraditionallyhastwosub-tasks. The\\nfirstistoclassifytheclaimaseither\"Supported\",\"Refuted\"or\"NotEnoughInfo\",whichisthetask\\nweexploreinthemainpaper. FEVER’sothersub-taskinvolvesextractingsentencesfromWikipedia\\nasevidencesupportingtheclassificationprediction. AsFEVERusesadifferentWikipediadumpto\\nus,directlytacklingthistaskisnotstraightforward. Wehopetoaddressthisinfuturework.\\nF NullDocumentProbabilities\\nWeexperimentedwithadding\"Nulldocument\"mechanismtoRAG,similartoREALM[20]inorder\\ntomodelcaseswherenousefulinformationcouldberetrievedforagiveninput. Here,ifkdocuments\\nwereretrieved,wewouldadditionally\"retrieve\"anemptydocumentandpredictalogitforthenull\\ndocument,beforemarginalizingoverk+1predictions. Weexploredmodellingthisnulldocument\\nlogitbylearning(i)adocumentembeddingforthenulldocument,(ii)astaticlearntbiasterm,or\\n(iii)aneuralnetworktopredictthelogit. Wedidnotfindthattheseimprovedperformance,soin\\ntheinterestsofsimplicity,weomitthem. ForOpenMS-MARCO,whereusefulretrieveddocuments\\ncannotalwaysberetrieved,weobservethatthemodellearnstoalwaysretrieveaparticularsetof\\ndocumentsforquestionsthatarelesslikelytobenefitfromretrieval,suggestingthatnulldocument\\nmechanismsmaynotbenecessaryforRAG.\\nG Parameters\\nOurRAGmodelscontainthetrainableparametersfortheBERT-basequeryanddocumentencoderof\\nDPR,with110Mparameterseach(althoughwedonottrainthedocumentencoderourselves)and\\n406MtrainableparametersfromBART-large,406Mparameters,makingatotalof626Mtrainable\\n18\\n'),\n",
       " Document(metadata={'source': './2005.11401.pdf', 'file_path': './2005.11401.pdf', 'page': 18, 'total_pages': 19, 'Author': '', 'CreationDate': 'D:20210413004838Z', 'Creator': 'LaTeX with hyperref', 'Keywords': '', 'ModDate': 'D:20210413004838Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'Producer': 'pdfTeX-1.40.21', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Table7: Numberofinstancesinthedatasetsused. *Ahiddensubsetofthisdataisusedforevaluation\\nTask Train Development Test\\nNaturalQuestions 79169 8758 3611\\nTriviaQA 78786 8838 11314\\nWebQuestions 3418 362 2033\\nCuratedTrec 635 134 635\\nJeopardyQuestionGeneration 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. Thebestperforming\"closed-book\"(parametriconly)open-domainQAmodelisT5-11B\\nwith11Billiontrainableparameters. TheT5modelwiththeclosestnumberofparameterstoour\\nmodelsisT5-large(770Mparameters),whichachievesascoreof28.9EMonNaturalQuestions[52],\\nsubstantiallybelowthe44.5thatRAG-Sequenceachieves,indicatingthathybridparametric/non-\\nparametricmodelsrequirefarfewertrainableparametersforstrongopen-domainQAperformance.\\nThenon-parametricmemoryindexdoesnotconsistoftrainableparameters,butdoesconsistsof21M\\n728dimensionalvectors,consistingof15.3Bvalues. Thesecanbeeasilybestoredat8-bitfloating\\npointprecisiontomanagememoryanddiskfootprints.\\nH RetrievalCollapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\\nretrievalcomponentwould“collapse”andlearntoretrievethesamedocumentsregardlessofthe\\ninput. Inthesecases,onceretrievalhadcollapsed,thegeneratorwouldlearntoignorethedocuments,\\nandtheRAGmodelwouldperformequivalentlytoBART.Thecollapsecouldbeduetoaless-explicit\\nrequirementforfactualknowledgeinsometasks,orthelongertargetsequences,whichcouldresult\\ninlessinformativegradientsfortheretriever. Perezetal.[46]alsofoundspuriousretrievalresults\\nwhenoptimizingaretrievalcomponentinordertoimproveperformanceondownstreamtasks.\\nI Numberofinstancesperdataset\\nThenumberoftraining,developmentandtestdatapointsineachofourdatasetsisshowninTable7.\\n19\\n')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1ef09-653a-44e5-8aff-0f9b0d9eb257",
   "metadata": {},
   "source": [
    "## Knowledge source에서 text 추출\n",
    "\n",
    "### chunk size 1000, overlap 200으로 설정\n",
    "#### chunk_overlap\n",
    "  - 인접한 두 chunk가 겹치도록 설정하여 context를 유지\n",
    "\n",
    "### Vector store\n",
    "#### - Chroma로 vector store 생성\n",
    "#### - vectorstore는 text와 embedding의 pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b918e6f4-614b-49c6-b342-aa8c1b0ad9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # 적당한 크기 조정 가능\n",
    "    chunk_overlap=200\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82463fd2-0761-455f-b14d-ce7ee6d96ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs, \n",
    "    embedding=OpenAIEmbeddings(api_key=api_key),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e75a5c9-b735-42ea-8e41-6fa2fb220e17",
   "metadata": {},
   "source": [
    "## 질문과 유사한 embedding을 갖고 있는 text를 추출하는 코드\n",
    "\n",
    "### vector store에서 retriever 생성\n",
    "#### top k 설정하기\n",
    "  - 검색해서 return할 유사한 문서, chunk의 수를 결정하는 부분\n",
    "  - default는 4로 되어 있음\n",
    "\n",
    "##### k값을 늘리는 것의 의미\n",
    "  - k값이 작으면 문서가 여러 chunk로 쪼개진 경우 앞 부분만 검색될 수 있는데, 이 부분을 방지할 수 있음\n",
    "  - k값이 작으면 문서의 일부만 보고 편향된 응답을 할 수 있는데 그럴 가능성이 낮아짐\n",
    "  - k값이 너무 크면 관련없는 정보까지 포함될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1124bb6a-f049-44e9-ab63-0bbce130fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca87545e-1a96-4cd9-9d1b-febb838966e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "user_msg = \"이 논문 요약해줘.\"\n",
    "retrieved_docs = retriever.invoke(user_msg)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760379c-0492-46bb-9164-62fc4e945aa7",
   "metadata": {},
   "source": [
    "### custom_prompt 설정\n",
    "#### 1. 논문을 요약 전문가로 설정\n",
    "#### 2. 요약에 포함할 내용\n",
    "##### 1) 논문의 의미\n",
    "##### 2) 논문의 method나 접근 방버\n",
    "##### 3) 주요 발견 및 결과\n",
    "##### 4) 논문의 인사이트\n",
    "#### 3. 문맥에 없는 사실을 만들어내지 말 것\n",
    "#### 4. reference나 인용은 포함시키지 말 것\n",
    "#### 5. 명확하고 학술적인 언어로 작성할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3513306d-42cf-4006-afa0-258d657679db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "custom_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a research assistant specialized in summarizing academic papers.\n",
    "\n",
    "Given the following excerpts retrieved from a research paper, please generate a concise and comprehensive summary that captures:\n",
    "- The main contribution of the paper\n",
    "- The methodology or proposed approach\n",
    "- The key findings or results\n",
    "- Any unique insights or implications\n",
    "\n",
    "Do not make up facts that are not present in the context. Do not include references or citations.\n",
    "\n",
    "Use clear and formal academic language. Write in one paragraph. The summary should be understandable to someone familiar with NLP and machine learning.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "193b053f-74d4-48e1-8c06-4c7b9895593e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='\\nYou are a research assistant specialized in summarizing academic papers.\\n\\nGiven the following excerpts retrieved from a research paper, please generate a concise and comprehensive summary that captures:\\n- The main contribution of the paper\\n- The methodology or proposed approach\\n- The key findings or results\\n- Any unique insights or implications\\n\\nDo not make up facts that are not present in the context. Do not include references or citations.\\n\\nUse clear and formal academic language. Write in one paragraph. The summary should be understandable to someone familiar with NLP and machine learning.\\n\\nContext:\\n[7] ChristopherClarkandMattGardner. SimpleandEffectiveMulti-ParagraphReadingCompre-\\nhension. arXiv:1710.10723[cs],October2017. URLhttp://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingof\\nDeepBidirectionalTransformersforLanguageUnderstanding. InProceedingsofthe2019Con-\\nferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human\\nLanguageTechnologies,Volume1(LongandShortPapers),pages4171–4186,Minneapolis,\\nMinnesota,June2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/N19-1423.\\nURLhttps://www.aclweb.org/anthology/N19-1423.\\n[9] EmilyDinan,StephenRoller,KurtShuster,AngelaFan,MichaelAuli,andJasonWeston. Wiz-\\nardofwikipedia: Knowledge-poweredconversationalagents. InInternationalConferenceon\\nLearningRepresentations,2019. URLhttps://openreview.net/forum?id=r1l73iRqKm.\\n[10] MatthewDunn,LeventSagun,MikeHiggins,V.UgurGuney,VolkanCirik,andKyunghyun\\n\\n[7] ChristopherClarkandMattGardner. SimpleandEffectiveMulti-ParagraphReadingCompre-\\nhension. arXiv:1710.10723[cs],October2017. URLhttp://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingof\\nDeepBidirectionalTransformersforLanguageUnderstanding. InProceedingsofthe2019Con-\\nferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human\\nLanguageTechnologies,Volume1(LongandShortPapers),pages4171–4186,Minneapolis,\\nMinnesota,June2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/N19-1423.\\nURLhttps://www.aclweb.org/anthology/N19-1423.\\n[9] EmilyDinan,StephenRoller,KurtShuster,AngelaFan,MichaelAuli,andJasonWeston. Wiz-\\nardofwikipedia: Knowledge-poweredconversationalagents. InInternationalConferenceon\\nLearningRepresentations,2019. URLhttps://openreview.net/forum?id=r1l73iRqKm.\\n[10] MatthewDunn,LeventSagun,MikeHiggins,V.UgurGuney,VolkanCirik,andKyunghyun\\n\\n[7] ChristopherClarkandMattGardner. SimpleandEffectiveMulti-ParagraphReadingCompre-\\nhension. arXiv:1710.10723[cs],October2017. URLhttp://arxiv.org/abs/1710.10723.\\narXiv: 1710.10723.\\n[8] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingof\\nDeepBidirectionalTransformersforLanguageUnderstanding. InProceedingsofthe2019Con-\\nferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human\\nLanguageTechnologies,Volume1(LongandShortPapers),pages4171–4186,Minneapolis,\\nMinnesota,June2019.AssociationforComputationalLinguistics. doi: 10.18653/v1/N19-1423.\\nURLhttps://www.aclweb.org/anthology/N19-1423.\\n[9] EmilyDinan,StephenRoller,KurtShuster,AngelaFan,MichaelAuli,andJasonWeston. Wiz-\\nardofwikipedia: Knowledge-poweredconversationalagents. InInternationalConferenceon\\nLearningRepresentations,2019. URLhttps://openreview.net/forum?id=r1l73iRqKm.\\n[10] MatthewDunn,LeventSagun,MikeHiggins,V.UgurGuney,VolkanCirik,andKyunghyun\\n\\nthenewsoronsocialmedia;toimpersonateothers;ortoautomatetheproductionofspam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncomingdecades[16]. Inordertomitigatetheserisks,AIsystemscouldbeemployedtofightagainst\\nmisleadingcontentandautomatedspam/phishing.\\nAcknowledgments\\nTheauthorswouldliketothankthereviewersfortheirthoughtfulandconstructivefeedbackonthis\\npaper,aswellasHuggingFacefortheirhelpinopen-sourcingcodetorunRAGmodels. Theauthors\\nwouldalsoliketothankKyunghyunChoandSewonMinforproductivediscussionsandadvice. EP\\nthankssupportsfromtheNSFGraduateResearchFellowship. PLissupportedbytheFAIRPhD\\nprogram.\\nReferences\\n[1] PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,Rangan\\nMajumder,AndrewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n\\nthenewsoronsocialmedia;toimpersonateothers;ortoautomatetheproductionofspam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncomingdecades[16]. Inordertomitigatetheserisks,AIsystemscouldbeemployedtofightagainst\\nmisleadingcontentandautomatedspam/phishing.\\nAcknowledgments\\nTheauthorswouldliketothankthereviewersfortheirthoughtfulandconstructivefeedbackonthis\\npaper,aswellasHuggingFacefortheirhelpinopen-sourcingcodetorunRAGmodels. Theauthors\\nwouldalsoliketothankKyunghyunChoandSewonMinforproductivediscussionsandadvice. EP\\nthankssupportsfromtheNSFGraduateResearchFellowship. PLissupportedbytheFAIRPhD\\nprogram.\\nReferences\\n[1] PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,Rangan\\nMajumder,AndrewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n\\nthenewsoronsocialmedia;toimpersonateothers;ortoautomatetheproductionofspam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncomingdecades[16]. Inordertomitigatetheserisks,AIsystemscouldbeemployedtofightagainst\\nmisleadingcontentandautomatedspam/phishing.\\nAcknowledgments\\nTheauthorswouldliketothankthereviewersfortheirthoughtfulandconstructivefeedbackonthis\\npaper,aswellasHuggingFacefortheirhelpinopen-sourcingcodetorunRAGmodels. Theauthors\\nwouldalsoliketothankKyunghyunChoandSewonMinforproductivediscussionsandadvice. EP\\nthankssupportsfromtheNSFGraduateResearchFellowship. PLissupportedbytheFAIRPhD\\nprogram.\\nReferences\\n[1] PayalBajaj,DanielCampos,NickCraswell,LiDeng,JianfengGao,XiaodongLiu,Rangan\\nMajumder,AndrewMcNamara,BhaskarMitra,TriNguyen,MirRosenberg,XiaSong,Alina\\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:\\n\\nS.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeural\\nInformationProcessingSystems30,pages5998–6008.CurranAssociates,Inc.,2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun,StefanLee,David\\nCrandall,andDhruvBatra. Diversebeamsearchforimproveddescriptionofcomplexscenes.\\nAAAIConferenceonArtificialIntelligence,2018. URLhttps://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeuralNetworksforNLP,pages353–355,Brussels,Belgium,November2018.Associationfor\\nComputationalLinguistics. doi: 10.18653/v1/W18-5446. URLhttps://www.aclweb.org/\\nanthology/W18-5446.\\n\\nS.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeural\\nInformationProcessingSystems30,pages5998–6008.CurranAssociates,Inc.,2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun,StefanLee,David\\nCrandall,andDhruvBatra. Diversebeamsearchforimproveddescriptionofcomplexscenes.\\nAAAIConferenceonArtificialIntelligence,2018. URLhttps://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeuralNetworksforNLP,pages353–355,Brussels,Belgium,November2018.Associationfor\\nComputationalLinguistics. doi: 10.18653/v1/W18-5446. URLhttps://www.aclweb.org/\\nanthology/W18-5446.\\n\\nS.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeural\\nInformationProcessingSystems30,pages5998–6008.CurranAssociates,Inc.,2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\\n[59] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun,StefanLee,David\\nCrandall,andDhruvBatra. Diversebeamsearchforimproveddescriptionofcomplexscenes.\\nAAAIConferenceonArtificialIntelligence,2018. URLhttps://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329.\\n[60] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeuralNetworksforNLP,pages353–355,Brussels,Belgium,November2018.Associationfor\\nComputationalLinguistics. doi: 10.18653/v1/W18-5446. URLhttps://www.aclweb.org/\\nanthology/W18-5446.\\n\\nwith\"TheSunAlsoRises\"isanovelbythisauthorof\"AFarewelltoArms\". Thisexampleshows\\nhowparametricandnon-parametricmemoriesworktogether—thenon-parametriccomponenthelps\\ntoguidethegeneration,drawingoutspecificknowledgestoredintheparametricmemory.\\n4.4 FactVerification\\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of\\nstate-of-the-artmodels,whicharecomplexpipelinesystemswithdomain-specificarchitecturesand\\nsubstantialengineering,trainedusingintermediateretrievalsupervision,whichRAGdoesnotrequire.\\n6\\n\\nAnswer:\\n'\n"
     ]
    }
   ],
   "source": [
    "user_prompt = custom_prompt.invoke({\"context\": format_docs(retrieved_docs), \"question\": user_msg})\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6e01c1f-0853-43af-a654-7d2c33ede8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper presents a novel approach to multi-paragraph reading comprehension by introducing the RAG (Retrieval-Augmented Generation) model, which effectively combines parametric and non-parametric memory components to improve performance in tasks requiring contextual understanding. The proposed methodology utilizes retrieval mechanisms to enhance the generation of responses based on external knowledge sources, allowing for better contextual relevance and accuracy. The key findings demonstrate that RAG achieves competitive results in reading comprehension benchmarks, outperforming traditional models without needing complex pipeline architectures or extensive retrieval supervision. Notably, the work highlights the potential of integrating retrieval methods into generative models, which can lead to advances in various natural language processing applications by leveraging external knowledge efficiently without significant increases in complexity.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(user_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381122b8-04d3-4eb4-8737-022ae2d24939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hhplus_week5)",
   "language": "python",
   "name": "hhplus_week5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

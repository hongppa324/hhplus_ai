{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1708c812-8bb5-45e5-9077-e16684f7ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 가져오기\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import wandb\n",
    "import logging\n",
    "import datasets\n",
    "import argparse\n",
    "import evaluate\n",
    "import transformers\n",
    "\n",
    "from typing import Optional\n",
    "from itertools import chain\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ac41eff9-9cd9-4bbc-8dd5-8db4e1e219ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt-finetuning-1</strong> at: <a href='https://wandb.ai/hongppa324-bizbee/Hanghae99/runs/6p9xspu8' target=\"_blank\">https://wandb.ai/hongppa324-bizbee/Hanghae99/runs/6p9xspu8</a><br> View project at: <a href='https://wandb.ai/hongppa324-bizbee/Hanghae99' target=\"_blank\">https://wandb.ai/hongppa324-bizbee/Hanghae99</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250511_143858-6p9xspu8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/seungchanhong/hhplus_week7/wandb/run-20250511_144003-bo6ig0b3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongppa324-bizbee/Hanghae99/runs/bo6ig0b3' target=\"_blank\">soft-terrain-35</a></strong> to <a href='https://wandb.ai/hongppa324-bizbee/Hanghae99' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongppa324-bizbee/Hanghae99' target=\"_blank\">https://wandb.ai/hongppa324-bizbee/Hanghae99</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongppa324-bizbee/Hanghae99/runs/bo6ig0b3' target=\"_blank\">https://wandb.ai/hongppa324-bizbee/Hanghae99/runs/bo6ig0b3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb 연결 후 project 생성\n",
    "wandb.init(project='Hanghae99')\n",
    "wandb.run.name = 'gpt-finetuning-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "51b01f67-60a1-4375-8893-4dae0c69f740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:2164] 2025-05-11 14:40:05,975 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1835] 2025-05-11 14:40:05,976 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Argument 정의\n",
    "@dataclass\n",
    "class Arguments:\n",
    "    model_name_or_path: Optional[str] = field(default=None)  # HuggingFace hub에서 pre-trained 모델로 사용할 모델의 이름\n",
    "    # 우리 모델의 precision(=> data type)\n",
    "    torch_dtype: Optional[str] = field(default=None, metadata={'choices': ['auto', 'bfloat16', 'float16', 'float32']})\n",
    "    dataset_name: Optional[str] = field(default=None)  # Fine-tuning으로 사용할 huggingface hub에서의 dataset 이름\n",
    "    dataset_config_name: Optional[str] = field(default=None)  # Fine-tuning으로 사용할 huggingface hub에서의 dataset configuration\n",
    "    block_size: int = field(default=1024)  # Fine-tuning에 사용할 input text의 길이\n",
    "    num_workers: Optional[int] = field(default=None)  # Data를 업로드하거나 전처리할 때 사용할 worker 숫자\n",
    "\n",
    "arg_dict = {\n",
    "    \"model_name_or_path\": \"gpt2\",\n",
    "    \"torch_dtype\": \"float32\",\n",
    "    \"dataset_name\": \"wikitext\",\n",
    "    \"dataset_config_name\": \"wikitext-2-raw-v1\",\n",
    "    \"block_size\": 128,\n",
    "    \"num_workers\": 2,\n",
    "    \"output_dir\": \"./gpt2-output\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"eval_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 2,\n",
    "    \"remove_unused_columns\": False\n",
    "}\n",
    "\n",
    "parser = HfArgumentParser((Arguments, TrainingArguments))\n",
    "args, training_args = parser.parse_dict(arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c8badf17-371c-4bd5-acad-609e41caf5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:05 - INFO - root - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.EPOCH,\n",
      "eval_use_gather_object=False,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./gpt2-output/runs/May11_14-40-05_hongseungchan-ui-MacBookPro.local,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.EPOCH,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./gpt2-output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./gpt2-output,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.EPOCH,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tp_size=0,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# logger 설정하기\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    transformers.utils.logging.set_verbosity_info()  # log level을 INFO로 변경 \n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "\n",
    "# 우리가 가지고 있는 logger와 HuggingFace의 logger의 log level 설정\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "\n",
    "# 기타 HuggingFace logger option들을 설정\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3509a034-a909-4ebb-99d7-b8193ea1be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:12 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:12 - INFO - datasets.info - Loading Dataset info from /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:12 - INFO - datasets.builder - Found cached dataset wikitext (/Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:12 - INFO - datasets.info - Loading Dataset info from /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "05/11/2025 14:40:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:12 - INFO - datasets.arrow_dataset - Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:12 - INFO - datasets.arrow_dataset - Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:12 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:12 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1930\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:693] 2025-05-11 14:40:12,970 >> loading configuration file config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-11 14:40:12,972 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:693] 2025-05-11 14:40:13,191 >> loading configuration file config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-11 14:40:13,193 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,197 >> loading file vocab.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,198 >> loading file merges.txt from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,199 >> loading file tokenizer.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,199 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,200 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,201 >> loading file tokenizer_config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,202 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:693] 2025-05-11 14:40:13,205 >> loading configuration file config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-11 14:40:13,206 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1124] 2025-05-11 14:40:13,275 >> loading weights file model.safetensors from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "[INFO|modeling_utils.py:2167] 2025-05-11 14:40:13,278 >> Instantiating GPT2LMHeadModel model under default dtype torch.float32.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-11 14:40:13,279 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4930] 2025-05-11 14:40:13,303 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4938] 2025-05-11 14:40:13,303 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:1097] 2025-05-11 14:40:13,514 >> loading configuration file generation_config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-11 14:40:13,516 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset 로드하기\n",
    "raw_datasets = load_dataset(\n",
    "    args.dataset_name,\n",
    "    args.dataset_config_name\n",
    ")\n",
    "\n",
    "print(raw_datasets)\n",
    "eval_dataset = raw_datasets[\"validation\"] if \"validation\" in raw_datasets else None\n",
    "if eval_dataset is not None:\n",
    "    tokenized_eval = eval_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset.column_names\n",
    "    )\n",
    "    eval_dataset = tokenized_eval.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=args.num_workers\n",
    "    )\n",
    "\n",
    "print(eval_dataset)\n",
    "\n",
    "# model config\n",
    "config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    config=config,\n",
    "    torch_dtype=args.torch_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "97d688ac-fff8-466d-9e64-53bbab88ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:693] 2025-05-11 14:40:13,737 >> loading configuration file config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-11 14:40:13,740 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,743 >> loading file vocab.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,744 >> loading file merges.txt from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,744 >> loading file tokenizer.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,745 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,745 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,745 >> loading file tokenizer_config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-05-11 14:40:13,746 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|configuration_utils.py:693] 2025-05-11 14:40:13,748 >> loading configuration file config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-11 14:40:13,750 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:693] 2025-05-11 14:40:14,079 >> loading configuration file config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-11 14:40:14,082 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1124] 2025-05-11 14:40:14,086 >> loading weights file model.safetensors from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "[INFO|configuration_utils.py:1142] 2025-05-11 14:40:14,089 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4930] 2025-05-11 14:40:14,128 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4938] 2025-05-11 14:40:14,129 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:1097] 2025-05-11 14:40:14,340 >> loading configuration file generation_config.json from cache at /Users/seungchanhong/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-11 14:40:14,342 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e5c2b46b2e42895b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e5c2b46b2e42895b.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-18838ae4e0f2bf1f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-18838ae4e0f2bf1f.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e5c2b46b2e42895b_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e5c2b46b2e42895b_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e5c2b46b2e42895b_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e5c2b46b2e42895b_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e5c2b46b2e42895b_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-e5c2b46b2e42895b_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-18838ae4e0f2bf1f_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-18838ae4e0f2bf1f_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-18838ae4e0f2bf1f_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-18838ae4e0f2bf1f_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-18838ae4e0f2bf1f_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-18838ae4e0f2bf1f_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-83d8f972563338fa_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-7a8cbaace6482f50_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-7a8cbaace6482f50_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-7a8cbaace6482f50_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-7a8cbaace6482f50_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-7a8cbaace6482f50_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-7a8cbaace6482f50_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6df5f404cf046b04_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6df5f404cf046b04_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6df5f404cf046b04_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6df5f404cf046b04_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6df5f404cf046b04_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-6df5f404cf046b04_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #0 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_00000_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Process #1 will write at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_00001_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /Users/seungchanhong/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3/cache-cc2549edca868b7c_*_of_00002.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 2 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/11/2025 14:40:14 - INFO - datasets.arrow_dataset - Concatenating 2 shards\n"
     ]
    }
   ],
   "source": [
    "# model config\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  \n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}\"\n",
    "\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    output = tokenizer(examples[text_column_name])\n",
    "    return output\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "    \n",
    "with training_args.main_process_first(desc=\"dataset map tokenization\"):\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=args.num_workers,\n",
    "        remove_columns=column_names\n",
    "    )\n",
    "\n",
    "max_pos_embeddings = config.max_position_embeddings if hasattr(config, \"max_position_embeddings\") else 1024\n",
    "block_size = args.block_size if tokenizer.model_max_length is None else min(args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "def group_texts(examples):\n",
    "    # 주어진 text들을 모두 concat 해줍니다. \n",
    "    # 예를 들어 examples = {'train': [['Hello!'], ['Yes, that is great!']]}이면 결과물은 {'train': ['Hello! Yes, that is great!']}가 됩니다.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    \n",
    "    # 전체 길이를 측정합니다.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # block_size로 text를 쪼갭니다.\n",
    "    # 예를 들어 block_size=3일 때 {'train': ['Hello! Yes, that is great!']}는\n",
    "    # {'train': ['Hel', 'lo!', ' Ye', 's, ', 'tha', ...]}가 됩니다. \n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Next token prediction이니 label은 자기 자신으로 설정합니다.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "    \n",
    "with training_args.main_process_first(desc=\"grouping texts together\"):\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=args.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8de57380-a86c-4102-a779-9e65c58f4dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/dp_hy1vd5jj8pzrlts8w9z4m0000gn/T/ipykernel_59391/1757776500.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "[INFO|trainer.py:2414] 2025-05-11 14:40:14,804 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-05-11 14:40:14,804 >>   Num examples = 18,667\n",
      "[INFO|trainer.py:2416] 2025-05-11 14:40:14,804 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2417] 2025-05-11 14:40:14,805 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2420] 2025-05-11 14:40:14,805 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2421] 2025-05-11 14:40:14,805 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2422] 2025-05-11 14:40:14,805 >>   Total optimization steps = 14,001\n",
      "[INFO|trainer.py:2423] 2025-05-11 14:40:14,805 >>   Number of trainable parameters = 124,439,808\n",
      "[INFO|integration_utils.py:831] 2025-05-11 14:40:14,806 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14001' max='14001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14001/14001 34:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.469500</td>\n",
       "      <td>3.408760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.158600</td>\n",
       "      <td>3.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.015100</td>\n",
       "      <td>3.413268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:4307] 2025-05-11 14:51:32,917 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4309] 2025-05-11 14:51:32,918 >>   Num examples = 1930\n",
      "[INFO|trainer.py:4312] 2025-05-11 14:51:32,918 >>   Batch size = 8\n",
      "[INFO|trainer.py:3984] 2025-05-11 14:51:49,076 >> Saving model checkpoint to ./gpt2-output/checkpoint-4667\n",
      "[INFO|configuration_utils.py:419] 2025-05-11 14:51:49,077 >> Configuration saved in ./gpt2-output/checkpoint-4667/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-05-11 14:51:49,077 >> Configuration saved in ./gpt2-output/checkpoint-4667/generation_config.json\n",
      "[INFO|modeling_utils.py:3572] 2025-05-11 14:51:49,298 >> Model weights saved in ./gpt2-output/checkpoint-4667/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-11 14:51:49,299 >> tokenizer config file saved in ./gpt2-output/checkpoint-4667/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-11 14:51:49,300 >> Special tokens file saved in ./gpt2-output/checkpoint-4667/special_tokens_map.json\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/hhplus_week7/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "[INFO|trainer.py:4307] 2025-05-11 15:03:09,422 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4309] 2025-05-11 15:03:09,422 >>   Num examples = 1930\n",
      "[INFO|trainer.py:4312] 2025-05-11 15:03:09,422 >>   Batch size = 8\n",
      "[INFO|trainer.py:3984] 2025-05-11 15:03:25,198 >> Saving model checkpoint to ./gpt2-output/checkpoint-9334\n",
      "[INFO|configuration_utils.py:419] 2025-05-11 15:03:25,199 >> Configuration saved in ./gpt2-output/checkpoint-9334/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-05-11 15:03:25,200 >> Configuration saved in ./gpt2-output/checkpoint-9334/generation_config.json\n",
      "[INFO|modeling_utils.py:3572] 2025-05-11 15:03:25,408 >> Model weights saved in ./gpt2-output/checkpoint-9334/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-11 15:03:25,409 >> tokenizer config file saved in ./gpt2-output/checkpoint-9334/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-11 15:03:25,409 >> Special tokens file saved in ./gpt2-output/checkpoint-9334/special_tokens_map.json\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/hhplus_week7/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "[INFO|trainer.py:4307] 2025-05-11 15:14:43,254 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4309] 2025-05-11 15:14:43,254 >>   Num examples = 1930\n",
      "[INFO|trainer.py:4312] 2025-05-11 15:14:43,254 >>   Batch size = 8\n",
      "[INFO|trainer.py:3984] 2025-05-11 15:14:59,026 >> Saving model checkpoint to ./gpt2-output/checkpoint-14001\n",
      "[INFO|configuration_utils.py:419] 2025-05-11 15:14:59,027 >> Configuration saved in ./gpt2-output/checkpoint-14001/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-05-11 15:14:59,027 >> Configuration saved in ./gpt2-output/checkpoint-14001/generation_config.json\n",
      "[INFO|modeling_utils.py:3572] 2025-05-11 15:14:59,218 >> Model weights saved in ./gpt2-output/checkpoint-14001/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-11 15:14:59,219 >> tokenizer config file saved in ./gpt2-output/checkpoint-14001/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-11 15:14:59,220 >> Special tokens file saved in ./gpt2-output/checkpoint-14001/special_tokens_map.json\n",
      "[INFO|trainer.py:4083] 2025-05-11 15:15:00,426 >> Deleting older checkpoint [gpt2-output/checkpoint-4667] due to args.save_total_limit\n",
      "[INFO|trainer.py:2681] 2025-05-11 15:15:00,435 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:3984] 2025-05-11 15:15:00,438 >> Saving model checkpoint to ./gpt2-output\n",
      "[INFO|configuration_utils.py:419] 2025-05-11 15:15:00,439 >> Configuration saved in ./gpt2-output/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-05-11 15:15:00,439 >> Configuration saved in ./gpt2-output/generation_config.json\n",
      "[INFO|modeling_utils.py:3572] 2025-05-11 15:15:00,660 >> Model weights saved in ./gpt2-output/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-11 15:15:00,661 >> tokenizer config file saved in ./gpt2-output/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-11 15:15:00,661 >> Special tokens file saved in ./gpt2-output/special_tokens_map.json\n",
      "[INFO|trainer.py:4307] 2025-05-11 15:15:00,686 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4309] 2025-05-11 15:15:00,686 >>   Num examples = 1930\n",
      "[INFO|trainer.py:4312] 2025-05-11 15:15:00,687 >>   Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =  3406921GF\n",
      "  train_loss               =     3.2144\n",
      "  train_runtime            = 0:34:45.63\n",
      "  train_samples_per_second =     26.851\n",
      "  train_steps_per_second   =      6.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/hhplus_week7/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='242' max='242' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [242/242 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =     3.4133\n",
      "  eval_runtime            = 0:00:15.78\n",
      "  eval_samples_per_second =    122.288\n",
      "  eval_steps_per_second   =     15.334\n"
     ]
    }
   ],
   "source": [
    "train_dataset = lm_datasets[\"train\"]\n",
    "\n",
    "# trainer에 validation_dataset 추가\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n",
    "checkpoint = None\n",
    "\n",
    "# 만약 output_dir에 checkpoint가 남아있으면 이를 사용하고, 없으면 None이 return됨\n",
    "last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "# output_dir이 아닌 다른 위치에서의 checkpoint를 resume_from_checkpoint로 지정할 수 있음\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "else:  \n",
    "    # 아니면 last_checkpoint로 checkpoint를 지정함  \n",
    "    checkpoint = last_checkpoint\n",
    "    \n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "\n",
    "trainer.save_model()\n",
    "\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "if eval_dataset is not None:\n",
    "    eval_metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "trainer.save_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hhplus_week7]",
   "language": "python",
   "name": "conda-env-hhplus_week7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

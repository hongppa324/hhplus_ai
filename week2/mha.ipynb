{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "4f7e7b00-96db-4741-922b-ec2eae19efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a753651c-2bb5-4f14-ab9a-82790dc7e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5%]\")\n",
    "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test[:5%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "9ae14af6-cb90-47bd-8c82-63cf396b8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e3ac33ee-37b9-4f3d-884c-79a33072a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  max_len = 400\n",
    "  texts, labels = [], []\n",
    "  for row in batch:\n",
    "    labels.append(row['label'])\n",
    "    texts.append(row['text'])\n",
    "\n",
    "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=32, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "f2693afa-716c-4209-9bcd-fe21b927da83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts shape: torch.Size([32, 400])\n",
      "labels shape: torch.Size([32])\n",
      "texts (token IDs): tensor([[  101,  2043,  2097,  1996, 11878,  2644,  1029,  1045,  2196,  2215,\n",
      "          2000,  2156,  2178,  2544,  1997,  1037,  4234,  8594,  2153,  1012,\n",
      "          2027,  2562,  2006,  2437,  5691,  2007,  1996,  2168,  2466,  1010,\n",
      "          4634,  2058,  2169,  2060,  1999,  2667,  2000,  2191,  1996,  3185,\n",
      "          2488,  2059,  1996,  2717,  1010,  2021, 13718,  8246,  2000,  2079,\n",
      "          2061,  1010,  2004,  2023,  2003,  2025,  1037,  2204,  2466,  1012,\n",
      "          7191,  6553,  1010,  2214,  1011, 13405,  1010,  4603,  3407,  1011,\n",
      "          3241,  1012,  2004,  2065,  2111,  4553,  1012,  1996,  3365,  2367,\n",
      "          4617,  1997,  2023,  2143,  6011,  2008,  2057,  2123, 29658,  2102,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  2347,  1005,  1056,  2469,  2129,  2000,  3446,  2023,\n",
      "          3185,  1010,  2144,  2009,  2001,  2061,  2919,  2009,  2001,  2941,\n",
      "          2200,  6057,  1012,  1045,  1005,  1049,  2025,  1037, 11721,  3600,\n",
      "          2102,  5470,  2011,  2151,  2965,  1010,  2295,  2002,  2003, 10904,\n",
      "          1010,  2750,  1996,  6881, 13881,  2008,  4165,  2066,  1037,  4937,\n",
      "         21454,  2039,  1037,  2606,  7384,  1012,  1045,  2467,  2245, 11804,\n",
      "          2001, 10904,  2295,  1010,  4752,  2003,  2019,  5875,  2201,  1012,\n",
      "          1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2021,  2006,\n",
      "          8476,  2182, 12455,  1012,  2023,  3185,  2003,  9951,  1012,  2009,\n",
      "          1005,  1055,  2061,  2058,  1996,  2327,  1998,  2512,  5054, 19570,\n",
      "          2389,  2009,  1005,  1055,  2471,  2066,  1037, 12354,  1997, 11189,\n",
      "          2895,  3152,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
      "          1028,  1996,  3185,  2038,  2471,  2053,  5436,  2182,  1010,  3272,\n",
      "          2009,  1005,  1055,  2074,  2055,  6144,  2007, 20067,  2814,  1012,\n",
      "          1999,  1037,  2126,  1010,  2023,  2143,  2471,  6966,  2033,  1997,\n",
      "          6804,  1011,  2158,  1017,  1010,  2007,  2129,  2045,  2020,  2205,\n",
      "          2116,  4784,  1010,  2029,  4504,  1999,  2025,  2438,  2051,  2000,\n",
      "          3477,  3086,  2006,  2028,  1997,  2068,  1012,  1026,  7987,  1013,\n",
      "          1028,  1026,  7987,  1013,  1028,  1996,  2895,  5019,  2020,  4756,\n",
      "          3085,  1012,  2855,  5493,  1010,  2471,  2524,  2000,  3305,  1010,\n",
      "          2007, 16967,  2008,  1005,  1055,  2061,  4756,  8231,  2919,  1012,\n",
      "          2295, 11804,  2246,  2200,  2358,  8516,  4509,  2076,  1996,  2895,\n",
      "          5019,  1010,  2021,  2008,  1005,  1055,  2023,  2143,  1005,  1055,\n",
      "          2069,  2107, 18434,  1012,  1045,  1005,  1049,  1037, 26476,  2005,\n",
      "          2204,  2895,  5691,  1010,  2021,  1996,  2895,  2001, 27762,  2589,\n",
      "          1012,  2295,  1996,  2345, 18297,  2001,  7929,  1998,  1996, 12944,\n",
      "          1997,  2023,  4728,  2139, 24128,  3185,  1012,  1026,  7987,  1013,\n",
      "          1028,  1026,  7987,  1013,  1028,  2009,  7906,  8660,  2090, 11541,\n",
      "          1010,  2025,  1037,  2204,  2518,  1012,  2009,  4122,  2000,  2022,\n",
      "          1037,  3689,  1010,  2030,  2019,  2895, 17312,  1010,  2030,  1037,\n",
      "          5469,  1010,  2030,  1037,  7472,  1012,  1012,  1012,  2054,  1996,\n",
      "          3109,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,\n",
      "          2065,  2023,  3319,  2003,  2437,  2017,  5506,  1010,  2339,  1029,\n",
      "          2003,  2009,  2138, 11721,  3600,  2102,  1998, 11804,  2024,  2115,\n",
      "          2293,  1029,  2123,  1005,  1056,  7966,  4426,  1010,  2023,  3185,\n",
      "          2003,  2919,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# text, label 확인하기\n",
    "for texts, labels in train_loader:\n",
    "    print(\"texts shape:\", texts.shape)\n",
    "    print(\"labels shape:\", labels.shape)\n",
    "    print(\"texts (token IDs):\", texts[:2])\n",
    "    print(\"labels:\", labels[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d91c4289-6ca8-4c7f-b039-59b25e81dd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "02180ff6-1c21-4c42-8b82-015332bee08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelfAttention Module을 Multi-head attention으로 확장\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0 # model의 차원은 head의 수로 나누어 떨어져야함\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # print(\"x shape\", x.shape)\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x) # (B, S, D)\n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"k shape\", k.shape)\n",
    "        # print(\"v shape\", v.shape)\n",
    "\n",
    "        # Q, K, V (B, S, D)를 (B, S, H, D')로 reshape \n",
    "        # D = H X D' => D' = D / H = d_head\n",
    "\n",
    "        # [step 1] (B, S, D) -> (B, H, S, D')\n",
    "        q = q.view(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.n_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # print(\"q reshaped:\", q.shape)\n",
    "        # print(\"k reshaped:\", k.shape)\n",
    "        # print(\"v reshaped:\", v.shape)\n",
    "\n",
    "        # [step 2] Attention score : (B, H, S, D') X (B, H, D', S) = (B, H, S, S)\n",
    "        score = torch.matmul(q, k.transpose(-1, -2)) / sqrt(self.d_head)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            score = score + (mask * -1e9)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "        score = self.dropout(score)\n",
    "        result = torch.matmul(score, v)\n",
    "\n",
    "        # transpose(1, 2)하고 나면 (B, S, H, D')\n",
    "        # 다시 (S, D)로 reshape\n",
    "        # contiguous()는 transpose하고 나서 tensor의 연속성을 보장해주기위해 사용함 (안 하면 오류 발생)\n",
    "        result = result.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # (B, S, D)\n",
    "        return self.dense(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054491a-863e-461c-86af-7fa4eadccb48",
   "metadata": {},
   "source": [
    "* contiguous를 안 넣어줬을 때  \n",
    "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d2ade7ff-7830-423a-9071-b231d7587071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dff, d_model)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        output = self.mha(x, mask)\n",
    "        \n",
    "        x1 = self.dropout1(output)\n",
    "        x1 = self.norm1(x + x1)\n",
    "\n",
    "        x2 = self.ffn(x1)\n",
    "        x2 = self.dropout2(x2)\n",
    "        x2 = self.norm2(x2 + x1)\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ddad7231-656e-417f-a44a-c04213eacaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400, 256])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "max_len = 400\n",
    "print(positional_encoding(max_len, 256).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f27ed8ac-29f2-4128-9696-c53072b7f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, dff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.dff = dff\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model, n_heads=4, dff=dff, dropout=dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.classification = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x == tokenizer.pad_token_id)\n",
    "        mask = mask[:, None, :]\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        x = self.embedding(x)\n",
    "        x = x * sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = x[:, 0]\n",
    "        x = self.classification(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c8890d4f-b3be-4119-aeb5-b7b9ed0f0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(len(tokenizer), d_model=64, n_layers=5, dff=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "cfe8a5dc-3ecd-4ae4-b5aa-8c60f9582c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_built())\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "683f62be-a5bb-4b11-b1bd-8035f289a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.001\n",
    "model = model.to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "a3115321-bb18-4630-92eb-ac84428e8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.float().unsqueeze(1)  # (B, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0).long()\n",
    "\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0ceceeb5-f7f5-416a-916c-054d08b6c2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6040689274668694\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 2 | Train Loss: 0.1068037748336792\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 3 | Train Loss: 0.05715763568878174\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 4 | Train Loss: 0.03455835580825806\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 5 | Train Loss: 0.02263106405735016\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 6 | Train Loss: 0.01572476327419281\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 7 | Train Loss: 0.011381357908248901\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 8 | Train Loss: 0.008488595485687256\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 9 | Train Loss: 0.00651174783706665\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 10 | Train Loss: 0.005070596933364868\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 11 | Train Loss: 0.004045933485031128\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 12 | Train Loss: 0.0032656490802764893\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 13 | Train Loss: 0.0026710331439971924\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 14 | Train Loss: 0.0022288262844085693\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 15 | Train Loss: 0.001862943172454834\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 16 | Train Loss: 0.0015708208084106445\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 17 | Train Loss: 0.0013470649719238281\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 18 | Train Loss: 0.00115242600440979\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 19 | Train Loss: 0.0010023415088653564\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 20 | Train Loss: 0.0008804798126220703\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 21 | Train Loss: 0.0007679760456085205\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 22 | Train Loss: 0.0006778836250305176\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 23 | Train Loss: 0.0006079673767089844\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 24 | Train Loss: 0.000536799430847168\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 25 | Train Loss: 0.0004811286926269531\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 26 | Train Loss: 0.00043505430221557617\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 27 | Train Loss: 0.00039139389991760254\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 28 | Train Loss: 0.0003598034381866455\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 29 | Train Loss: 0.00032570958137512207\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 30 | Train Loss: 0.00029143691062927246\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 31 | Train Loss: 0.00027051568031311035\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 32 | Train Loss: 0.00024646520614624023\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 33 | Train Loss: 0.00023186206817626953\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 34 | Train Loss: 0.00020566582679748535\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 35 | Train Loss: 0.00019800662994384766\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 36 | Train Loss: 0.0001786351203918457\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 37 | Train Loss: 0.00016349554061889648\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 38 | Train Loss: 0.00016030669212341309\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 39 | Train Loss: 0.0001499652862548828\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 40 | Train Loss: 0.00013005733489990234\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 41 | Train Loss: 0.0001227855682373047\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 42 | Train Loss: 0.00012022256851196289\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 43 | Train Loss: 0.00012129545211791992\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 44 | Train Loss: 0.00011551380157470703\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 45 | Train Loss: 9.295344352722168e-05\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 46 | Train Loss: 8.299946784973145e-05\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 47 | Train Loss: 8.270144462585449e-05\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 48 | Train Loss: 8.100271224975586e-05\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 49 | Train Loss: 7.933378219604492e-05\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n",
      "Epoch 50 | Train Loss: 7.82012939453125e-05\n",
      "=====> Train acc: 1.000 | Test acc: 1.000\n"
     ]
    }
   ],
   "source": [
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        model.zero_grad()\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # print(\"labels\", labels[:0])\n",
    "\n",
    "        labels = labels.float().unsqueeze(1)\n",
    "        \n",
    "        preds = model(inputs)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} | Train Loss: {total_loss}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_acc = accuracy(model, train_loader)\n",
    "        test_acc = accuracy(model, test_loader)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'=====> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d7d2e-44c4-47f1-bc2a-9e1def36e796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hhplus_week2)",
   "language": "python",
   "name": "hhplus_week2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "43ee2051-0057-4d75-be2e-99b288b50bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: sacremoses in /opt/anaconda3/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.12/site-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "0aa862b0-827c-4450-ada0-8b64fe5ee411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "8bd86930-1d6f-466f-84e2-f219af1436a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train[:5%]\")\n",
    "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test[:5%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "74b508fc-5001-4b4c-995d-210f81c9479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "b48766f2-2a6f-4771-a276-98a379ee74e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  max_len = 400\n",
    "  texts, labels = [], []\n",
    "  for row in batch:\n",
    "    labels.append(row['label'])\n",
    "    texts.append(row['text'])\n",
    "\n",
    "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, labels\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=16, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "# Out of memory 오류로 batch size 줄임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "baa5b0a1-83be-42df-8889-e56046a587cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts shape: torch.Size([16, 400])\n",
      "labels shape: torch.Size([16])\n",
      "texts (token IDs): tensor([[  101,  2004,  1037,  2309,  2450,  2058,  2871,  1010,  1045,  2179,\n",
      "          2023,  2143,  5186, 23979,  1998, 17183, 11219,  2075,  2000,  2309,\n",
      "          2308,  2058,  2871,  1010,  2025,  2000,  5254,  2296,  2060,  2450,\n",
      "          1010,  1997,  2151,  2287,  1012,  2009,  2001,  1037,  6517,  1010,\n",
      "         17203,  3535,  2011,  1037,  2158,  2000,  4339,  1998,  3622,  1037,\n",
      "          1000, 14556, 17312,  1000,  1010,  1998,  2009,  3478, 28616,  6906,\n",
      "          6321,  1012,  5557, 25005,  3475,  1005,  1056,  2172,  1997,  2019,\n",
      "          3883,  2000,  4088,  2007,  1010,  2021,  2445,  1996,  2512,  1011,\n",
      "         25953,  1000,  5436,  1000,  1006,  1045,  5223,  2000,  2130,  6523,\n",
      "          2000,  2009,  2004,  1037,  5436,  1007,  1999,  2023,  1010,  2016,\n",
      "          2134,  1005,  1056,  2031,  1037,  3382,  1012,  2045,  2001,  2053,\n",
      "          2839,  2458,  1010,  2053,  3114,  2000,  2514, 11883,  1013, 26452,\n",
      "          2005,  2151,  1997,  1996,  3494,  1010,  1998,  2053,  3535,  2000,\n",
      "          2191,  1996,  2143,  1999,  2151,  2126, 12689,  2030, 19337,  2666,\n",
      "         12423,  1012,  1998,  2059,  2045,  1005,  1055,  1996, 26471,  3287,\n",
      "          1011,  5913,  1997,  2019,  8702,  3442,  2450,  3402, 10561,  2000,\n",
      "          2507, 11690,  2964,  1037,  3046,  1011,  1011,  3531,  1012,  1026,\n",
      "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  2025,  2069,  2079,\n",
      "          1045,  4299,  1045,  2071,  2131,  2026,  2769,  2067,  2005,  1996,\n",
      "          4966, 12635,  1010,  1045,  2036,  2215,  2216, 11176,  2781,  1997,\n",
      "          2026,  2166,  2067,  1012,  2054,  1037, 10973,  7245,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037, 21686,  2466,  2081,  2005,  6001,  2040,  2444,  1999,\n",
      "          3959,  3122,  1012,  1996,  7472,  2003,  2200,  4895, 22852,  6553,\n",
      "          1010, 23638,  2100,  1010,  2293,  2100, 10855,  2100,  1010,  3819,\n",
      "          4385,  1012,  1996, 21686,  5436,  6229,  1996,  2200,  2203,  1998,\n",
      "          7890,  3593, 17129,  2003,  1996,  2069,  3114,  2005,  2026,  3340,\n",
      "          1012,  2065,  2017,  1005,  2128,  2559,  2005,  1037,  3959,  2100,\n",
      "          7472,  2007,  1037,  9792,  1010,  2023,  2003,  5791,  2017,  1005,\n",
      "          2128,  3185,  1010,  2021,  2005,  1996,  2717,  1997,  2149,  2613,\n",
      "          2088,  2111,  1010,  1045,  1005,  1040,  3811, 16755,  7494,  2115,\n",
      "          2093,  3178,  3422,  2051,  1012,  5256,  2039,  2111,   999,  1026,\n",
      "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  2176,  2041,  1997,\n",
      "          1996,  2274,  2111,  2008,  2387,  1996,  2143,  2007,  2033,  2052,\n",
      "          2025, 16755,  1996,  2143,  1012,  2057,  2018,  1037,  2307,  2051,\n",
      "         24234,  2075,  3484,  1997,  1996,  4895, 22852,  6553,  5019,  1012,\n",
      "          2672,  1045,  1005,  1049,  4394,  2242,  1012,  1012,  1045,  2074,\n",
      "          2064,  1005,  1056,  2903,  1037,  3185,  2066,  2023,  2064,  3786,\n",
      "          1037,  4438,  2066, 10751, 16409,  2015,   999,   999,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# text, label 확인하기\n",
    "for texts, labels in train_loader:\n",
    "    print(\"texts shape:\", texts.shape)\n",
    "    print(\"labels shape:\", labels.shape)\n",
    "    print(\"texts (token IDs):\", texts[:2])\n",
    "    print(\"labels:\", labels[:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "e93cafe7-bd4b-4d59-9408-d6b3e100caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "28070509-cf11-47d3-89e4-a70418b135f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SelfAttention(nn.Module):\n",
    "#   def __init__(self, input_dim, d_model):\n",
    "#     super().__init__()\n",
    "\n",
    "#     self.input_dim = input_dim\n",
    "#     self.d_model = d_model\n",
    "\n",
    "#     self.wq = nn.Linear(input_dim, d_model)\n",
    "#     self.wk = nn.Linear(input_dim, d_model)\n",
    "#     self.wv = nn.Linear(input_dim, d_model)\n",
    "#     self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "#     self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "#   def forward(self, x, mask):\n",
    "#     q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "#     score = torch.matmul(q, k.transpose(-1, -2))\n",
    "#     score = score / sqrt(self.d_model)\n",
    "\n",
    "#     if mask is not None:\n",
    "#       score = score + (mask * -1e9)\n",
    "\n",
    "#     score = self.softmax(score)\n",
    "#     result = torch.matmul(score, v)\n",
    "#     result = self.dense(result)\n",
    "\n",
    "#     return result\n",
    "\n",
    "# SelfAttention Module을 Multi-head attention으로 확장\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0 # model의 차원은 head의 수로 나누어 떨어져야함\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        print(x.shape)\n",
    "        B, S, _ = x.shape\n",
    "        \n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x) # (B, S, D)\n",
    "        # print(\"q shape\", q.shape)\n",
    "        # print(\"k shape\", k.shape)\n",
    "        # print(\"v shape\", v.shape)\n",
    "\n",
    "        # Q, K, V (B, S, D)를 (B, S, H, D')로 reshape \n",
    "        # D = H X D' => D' = D / H = d_head\n",
    "\n",
    "        # [step 1] (B, S, D) -> (B, H, S, D')\n",
    "        q = q.view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(B, S, self.n_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # print(\"q reshaped:\", q.shape)\n",
    "        # print(\"k reshaped:\", k.shape)\n",
    "        # print(\"v reshaped:\", v.shape)\n",
    "\n",
    "        # [step 2] Attention score : (B, H, S, D') X (B, H, D', S) = (B, H, S, S)\n",
    "        score = torch.matmul(q, k.transpose(-1, -2)) / sqrt(self.d_head)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            score = score + (mask * -1e9)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "        # print(\"score\", score.shape)\n",
    "        \n",
    "        result = torch.matmul(score, v) # (B, H, S, S) X (B, H, S, D') = (B, H, S, D')\n",
    "        # print(\"result 1 shape\", result.shape)\n",
    "\n",
    "        # transpose(1, 2)하고 나면 (B, S, H, D')\n",
    "        # 다시 (S, D)로 reshape\n",
    "        # contiguous()는 transpose하고 나서 tensor의 연속성을 보장해주기위해 사용함 (안 하면 오류 발생)\n",
    "        result = result.transpose(1, 2).contiguous().view(B, S, self.d_model) \n",
    "        # print(\"result 2 shape\", result.shape)\n",
    "        result = self.dense(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "32349f11-aa67-4312-8fbe-063098f4ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "# 테스트 코드\n",
    "B, S, input_dim = 2, 5, 32\n",
    "x = torch.randn(B, S, input_dim)\n",
    "mha = MultiHeadAttention(input_dim=32, d_model=32, n_heads=2)\n",
    "_ = mha(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d954720-16c1-447a-b059-1ace0a574a15",
   "metadata": {},
   "source": [
    "* contiguous를 안 넣어줬을 때  \n",
    "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "7785ad0a-766c-4a28-a9a1-be13be224ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "  def __init__(self, input_dim, d_model, dff, n_heads, dropout=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.d_model = d_model\n",
    "    self.dff = dff\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.mha = MultiHeadAttention(input_dim, d_model, n_heads)\n",
    "    \n",
    "    self.ffn = nn.Sequential(\n",
    "      nn.Linear(d_model, dff),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(dff, d_model)\n",
    "    )\n",
    "\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "    self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "  def forward(self, x, mask):\n",
    "    x1 = self.mha(x, mask)\n",
    "    x1 = self.dropout1(x1)\n",
    "    x1 = self.norm1(x1 + x)\n",
    "\n",
    "    x2 = self.ffn(x1)\n",
    "    x2 = self.dropout2(x2)\n",
    "    x2 = self.norm2(x2 + x1)\n",
    "\n",
    "    return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "54d8c663-92db-41a7-944c-36bcef4d9e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 400, 256])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "max_len = 400\n",
    "print(positional_encoding(max_len, 256).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ececcfcb-e60f-444e-9fbe-558c0c4cabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, dff, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.dff = dff\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model, d_model, dff, n_heads) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.classification = nn.Linear(d_model, 2) # 출력 차원을 2 (0: 부정, 1: 긍정)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x == tokenizer.pad_token_id)\n",
    "        mask = mask[:, None, :]\n",
    "        # print(mask.shape)\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = x * sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x1 = x[:, -1]\n",
    "        # x2 = x[:, 0]\n",
    "        # print(x1, x2)\n",
    "        x1 = self.classification(x1)\n",
    "        # x2 = self.classification(x2)\n",
    "        return x1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "491e05c1-2465-475e-ad97-92aeb8e84ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-layer 4-head Transformer 모델\n",
    "model = TextClassifier(len(tokenizer), d_model=64, n_layers=5, dff=128, n_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "21bb4a9b-d5da-4645-b2b3-6b97aa9531ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_built())\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "eaedec19-711f-4e25-803a-db631ab44018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.001\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() # loss_fn 변경 : 다중 분류에 적합한 CrossEntropyLoss\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "62c4cebd-3e9c-4cbe-bf78-5bfcb8f1ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, dataloader):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    outputs = model(inputs) # 출력의 차원은 (B, vocab_size)\n",
    "    print(outputs.shape)\n",
    "    preds = torch.argmax(outputs, dim=-1) # 마지막 차원에서 가장 큰 값을 고름\n",
    "      \n",
    "    correct += (labels == preds).sum().item()\n",
    "    total += labels.size(0)\n",
    "\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "0ca08ab9-e724-44fb-a7ee-6e544fe91035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels tensor([], device='mps:0', dtype=torch.int64)\n",
      "torch.Size([16, 400, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 400, 64]' is invalid for input of size 6553600",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[433], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, labels[:\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 16\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, labels)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week2/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week2/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[428], line 29\u001b[0m, in \u001b[0;36mTextClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding[:, :seq_len]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, mask)\n\u001b[1;32m     31\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# x2 = x[:, 0]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(x1, x2)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week2/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week2/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[426], line 25\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m---> 25\u001b[0m   x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha(x, mask)\n\u001b[1;32m     26\u001b[0m   x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x1)\n\u001b[1;32m     27\u001b[0m   x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x1 \u001b[38;5;241m+\u001b[39m x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week2/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hhplus_week2/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[423], line 84\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     78\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(score, v) \u001b[38;5;66;03m# (B, H, S, S) X (B, H, S, D') = (B, H, S, D')\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# print(\"result 1 shape\", result.shape)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# transpose(1, 2)하고 나면 (B, S, H, D')\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 다시 (S, D)로 reshape\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# contiguous()는 transpose하고 나서 tensor의 연속성을 보장해주기위해 사용함 (안 하면 오류 발생)\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(B, S, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model) \n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# print(\"result 2 shape\", result.shape)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(result)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 400, 64]' is invalid for input of size 6553600"
     ]
    }
   ],
   "source": [
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        model.zero_grad()\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        print(\"labels\", labels[:0])\n",
    "\n",
    "        preds = model(inputs)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} | Train Loss: {total_loss}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        train_acc = accuracy(model, train_loader)\n",
    "        test_acc = accuracy(model, test_loader)\n",
    "        train_accs.append(train_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'=====> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hhplus_week2)",
   "language": "python",
   "name": "hhplus_week2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
